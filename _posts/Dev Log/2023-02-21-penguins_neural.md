---
title: "Penguins ì¸ê³µì‹ ê²½ë§"
excerpt: "Penguins ë°ì´í„°ë¡œ ì¸ê³µì‹ ê²½ë§ êµ¬ì„±í•˜ê¸°"

categories:
  - Dev Log

tags:
  - ê°œë°œì¼ì§€
  - ì½”ë”©
  - keras
  - Neural Network

use_math: true

header:
  teaser: /assets/images/etc/penguin_png.png

last_modified_at: 2023-02-21
---
 

<br><br>


![image]({{ site.url }}{{ site.baseurl }}/assets/images/etc/penguin_png.png){: .align-center width="70%"}  


<br><br>


<br><br>


# Penguins, ì¸ê³µì‹ ê²½ë§ êµ¬ì„±  


<br><br>


## ë„ì…  
>Penguins ë°ì´í„°ë¡œ ì¸ê³µì‹ ê²½ë§ ë§Œë“¤ê¸°  
ì¸ê³µì‹ ê²½ë§ì„ ê³µë¶€í•˜ëŠ”MNISTë¥¼ ìì£¼ ì‚¬ìš©í•˜ê³¤ í•˜ì§€ë§Œ, MNISTì˜ indexëŠ” 60,000ê°œë¡œ ì‚¬ì´ì¦ˆê°€ í¬ê¸° ë•Œë¬¸ì— í•™ìŠµì— ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤.  
ê·¸ë˜ì„œ ë°ì´í„° ì‚¬ì´ì¦ˆê°€ ì‘ê³  ê·€ì—¬ìš´ Penguins ë°ì´í„°ë¡œ ì¸ê³µì‹ ê²½ë§ì„ ì—°ìŠµí•˜ì


<br><br>  
 

## ë°ì´í„° ì „ì²˜ë¦¬  

### ë°ì´í„° ì½ì–´ì˜¤ê¸°  

```python
# Data Load
penguins = sns.load_dataset('penguins')
print(f"ğŸ¬ Rows, Columns : {penguins.shape}")
# ğŸ¬ Rows, Columns : (344, 7)
```

- ë°ì´í„°ëŠ” Seabornì—ì„œ ë¶ˆëŸ¬ì™”ë‹¤.
- ë°ì´í„°ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ [Seaborn ë§í¬ ì°¸ì¡°](https://github.com/allisonhorst/palmerpenguins) í•´ì£¼ì„¸ìš”

<br>

### íŠ¹ì„±ê³µí•™

- íŠ¹ë³„í•œ íŠ¹ì„±ê³µí•™ì€ í•˜ì§€ ì•ŠìŒ
- sexê°€ ê²°ì¸¡ì¸ ê²½ìš° 3ìœ¼ë¡œ ëŒ€ì²´
- ë‹¤ë¥¸ íŠ¹ì„±ì€ í‰ê· ìœ¼ë¡œ ëŒ€ì²´

```python
penguins_species = {
    'Adelie' : 0,
    'Gentoo' : 1,
    'Chinstrap' : 2,
}
penguins['species'] = penguins['species'].map(penguins_species)

penguins_island = {
    'Biscoe' : 0,
    'Dream' : 1,
    'Torgersen' : 2,
}
penguins['island'] = penguins['island'].map(penguins_island)

penguins_sex = {
    'Male' : 0,
    'Female' : 1,
    np.nan : 3,
}
penguins['sex'] = penguins['sex'].map(penguins_sex)

# NaN = Fill with Mean
for idx, col in enumerate(penguins.columns):
  penguins[col].fillna(penguins[col].mean(), inplace=True)
```

<br>

### ë°ì´í„° ë¶„í• 

```python
test_size = 0.2
x_train, x_test, y_train, y_test = train_test_split(penguins[features], penguins[target], test_size=test_size, random_state=rand_seed)
print(f'âœ¨Train Shape : {x_train.shape} / {y_train.shape}\nğŸ‰ Test Shape : {x_test.shape} / {y_test.shape}')
#âœ¨Train Shape : (275, 6) / (275, 1)
#ğŸ‰ Test Shape : (69, 6) / (69, 1)
```

<br>

### ì •ê·œí™”(Normalization)

```python
x_train = (x_train - np.min(x_train, axis='index')) / (np.max(x_train, axis='index') - np.min(x_train, axis='index'))
x_test  = ( x_test - np.min( x_test, axis='index')) / (np.max( x_test, axis='index') - np.min( x_test, axis='index'))
```


<br><br>


## íƒìƒ‰ì  ë°ì´í„° ë¶„ì„

### Seaborn ì˜ ë°ì´í„° ì†Œê°œë¡œ ëŒ€ì²´

![image](https://github.com/allisonhorst/palmerpenguins/raw/main/man/figures/culmen_depth.png){: .align-center width="60%"}  


![image](https://seaborn.pydata.org/_images/introduction_29_0.png){: .align-center width="80%"}  


<br><br>


## ëª¨ë¸ë§

### ê¸°ì¤€ëª¨ë¸ : Logistic Regression

- ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ëª¨ë¸ì„ ê¸°ì¤€ëª¨ë¸ë¡œ í•˜ì!

```python
log_model = LogisticRegression()
log_model.fit(x_train, y_train)
log_score = print_accuracy(log_model)
log_CM, fig = draw_CM(log_model)
# Train Accuracy : 0.978, Test Accuracy : 0.971
```

- ê¸°ì¤€ëª¨ë¸ ì •í™•ë„ê°€ ë¬´ë ¤ 97.1% ğŸ˜³ (í°ì¼ì´ë‹¤...)
- ë³´í†µì€ ê¸°ì¤€ëª¨ë¸ ì´ìƒì„ ëª©í‘œë¡œ í•˜ì§€ë§Œ ì˜¤ëŠ˜ì€ ê¸°ì¤€ëª¨ë¸ ë§Œí¼ì´ë¼ë„ ë‹¬ì„±í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ì

![image]({{ site.url }}{{ site.baseurl }}/assets/images/coding/penguins/logit_cm.png){: .align-center width="60%"}  

<br>

### ê¸°ë³¸ ì¸ê³µì‹ ê²½ë§ : ì€ë‹‰ì¸µ 0ê°œ

- ê°€ì¥ ê¸°ë³¸ì ì¸ ì¸ê³µì‹ ê²½ë§ì„ ë§Œë“¤ì–´ë³´ì

```python
model1 = tf.keras.Sequential([
    Dense(3, activation='softmax'),
    ])

model1.compile(metrics=['accuracy']
               , optimizer='adam'
               , loss='sparse_categorical_crossentropy'
               )

results1 = model1.fit(x_train, y_train
                      , epochs=5
                      , verbose=0
                      )

model1_score = print_accuracy(model1)
log_CM, fig = draw_CM(model1)
model1.summary()
```

- ì€ë‹‰ì¸µ ì—†ì´ ì¶œë ¥ì¦ì—ë§Œ í­ê·„ì¢…ë¥˜ 3ê°€ì§€ë§Œ ì§€ì •í•˜ê³ , ë‹¤ì¤‘ë¶„ë¥˜ í™œì„±í™”í•¨ìˆ˜ì¸ softmax ì§€ì •
- ìµœì†Œí•œì˜ í•™ìŠµì„ ìœ„í•˜ì—¬ epochsë¥¼ 5ë¡œ ì§€ì •
- ì…ë ¥ íŠ¹ì„± 7ê°œ, ì¶œë ¥ íƒ€ê²Ÿí´ë˜ìŠ¤ 3ê°œ, Total íŒŒë¼ë¯¸í„° 21ê°œ
- í‰ê°€ ì •í™•ë„ 53.6%...ğŸ˜­ğŸ˜¢ğŸ˜ (ë”¥ëŸ¬ë‹... ë²„ë ¤ì•¼ í•˜ë‚˜?)

```
# Train Accuracy : 0.436, Test Accuracy : 0.536
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_37 (Dense)            (None, 3)                 21        
                                                                 
=================================================================
Total params: 21
Trainable params: 21
Non-trainable params: 0
_________________________________________________________________
```

![image]({{ site.url }}{{ site.baseurl }}/assets/images/coding/penguins/nn_base1.png){: .align-center width="60%"} 

<br>

### ê¸°ë³¸ ì¸ê³µì‹ ê²½ë§ : ì€ë‹‰ì¸µ 1ê°œ

- ì€ë‹‰ì¸µì„ í•˜ë‚˜ ë§Œë“¤ì–´ ë³´ì

```python
model1 = tf.keras.Sequential([
    Dense(100, activation='relu', kernel_initializer='he_uniform'),
    Dense(3, activation='softmax'),
    ])

model1.compile(metrics=['accuracy']
               , optimizer='adam'
               , loss='sparse_categorical_crossentropy'
               )

results1 = model1.fit(x_train, y_train
                      , epochs=5
                      , verbose=0
                      )

model1_score = print_accuracy(model1)
log_CM, fig = draw_CM(model1)
model1.summary()
```

- í•˜ë‚˜ì˜ ì€ë‹‰ì¸µì— ë…¸ë“œ 100ê°œ, í™œì„±í™”í•¨ìˆ˜ëŠ” ReLU, ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ëŠ” 'he_uniform'ì„ ì§€ì •
- ì€ë‹‰ì¸µì´ ìƒê¸°ë©´ì„œ ì¶”ì •í•´ì•¼ í•  íŒŒë¼ë¯¸í„°ê°€ 1,003ê°œë¡œ ëŠ˜ì—ˆë‹¤.
- í‰ê°€ ì •í™•ë„ 76.8%!!

```
# Train Accuracy : 0.807, Test Accuracy : 0.768
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_38 (Dense)            (None, 100)               700       
                                                                 
 dense_39 (Dense)            (None, 3)                 303       
                                                                 
=================================================================
Total params: 1,003
Trainable params: 1,003
Non-trainable params: 0
```

![image]({{ site.url }}{{ site.baseurl }}/assets/images/coding/penguins/nn_base2.png){: .align-center width="60%"} 

<br>

### ê¸°ë³¸ ì¸ê³µì‹ ê²½ë§ : ì€ë‹‰ì¸µ 2ê°œ

- ì¼ë°˜ì ìœ¼ë¡œ ë”¥ëŸ¬ë‹ì´ë¼ í•˜ë©´ ì€ë‹‰ì¸µì´ 2ê°œ ì´ìƒì„ ì˜ë¯¸í•œë‹¤.
- ì€ë‹‰ì¸µì„ 2ê°œ ì¶”ê°€í•´ë³´ì

```python
model1 = tf.keras.Sequential([
    Dense(100, activation='relu', kernel_initializer='he_uniform'),
    Dense(100, activation='relu', kernel_initializer='he_uniform'),
    Dense(3, activation='softmax'),
    ])

model1.compile(metrics=['accuracy']
               , optimizer='adam'
               , loss='sparse_categorical_crossentropy'
               )

results1 = model1.fit(x_train, y_train
                      , epochs=5
                      , verbose=0
                      )

model1_score = print_accuracy(model1)
log_CM, fig = draw_CM(model1)
model1.summary()
```

- ë˜‘ê°™ì€ ì€ë‹‰ì¸µì„ 2ê°œë¡œ ëŠ˜ë ¸ë‹¤
- í‰ê°€ ì •í™•ë„ 95.7%ğŸ‰

```
# Train Accuracy : 0.971, Test Accuracy : 0.957
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_40 (Dense)            (None, 100)               700       
                                                                 
 dense_41 (Dense)            (None, 100)               10100     
                                                                 
 dense_42 (Dense)            (None, 3)                 303       
                                                                 
=================================================================
Total params: 11,103
Trainable params: 11,103
Non-trainable params: 0
```

![image]({{ site.url }}{{ site.baseurl }}/assets/images/coding/penguins/nn_base3.png){: .align-center width="60%"} 


- ì¸ê³µì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì´ 2ê°œ ì´ìƒìœ¼ë¡œ ëŠ˜ì–´ë‚˜ì§€ê¹ ì •í™•ë„ê°€ ê¸‰ ìƒìŠ¹í–ˆë‹¤


<br><br>


## ì¸ê³µì‹ ê²½ë§ ì‹¤í—˜

### ì‹¤í—˜1 : ì€ë‹‰ì¸µì˜ ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ ì„±ëŠ¥ì´ ì˜¤ëŠ˜ê¹Œ?

- ì€ë‹‰ì¸µ 1ê°œ ~ 99ê°œ ê¹Œì§€ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì •í™•ë„ë¥¼ ë¹„êµí•´ë³´ì

![image]({{ site.url }}{{ site.baseurl }}/assets/images/coding/penguins/test_hiddens.png){: .align-center width="60%"} 

- 2ê°œ ì´í›„ë¡œëŠ” ì„±ëŠ¥ì˜ í–¥ìƒì´ í¬ê²Œ ì—†ëŠ” ê²ƒ ê°™ë‹¤
- ì•ìœ¼ë¡œëŠ” íš¨ìœ¨ì„±ì„ ìœ„í•˜ì—¬ ì€ë‹‰ì¸µì€ 2ê°œë¡œ ê³ ì •í•  ê²ƒì´ë‹¤

<br>

### ì‹¤í—˜2 : ì€ë‹‰ì¸µì˜ ë…¸ë“œë¥¼ ëŠ˜ë¦¬ë©´ ì„±ëŠ¥ì´ ì˜¤ë¥¼ê¹Œ?

- ì€ë‹‰ì¸µì˜ ë…¸ë“œëŠ” ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì˜ ê°€ì¤‘í•© ì—°ì‚°ì´ ì¼ì–´ë‚˜ëŠ” ë¶€ë¶„ì´ë‹¤.

![image]({{ site.url }}{{ site.baseurl }}/assets/images/coding/penguins/test_node.png){: .align-center width="60%"} 

- ì€ë‹‰ì¸µì´ ì˜¤ë¥´ë©´ì„œ ì„±ëŠ¥ë„ ì˜¤ë¥´ê³ , ì„±ëŠ¥ì˜ ì•ˆì •ê°ë„ ì˜¤ë¥´ëŠ” ê²ƒ ê°™ë‹¤.
- ë…¸ë“œ 80 ì´í›„ë¡œëŠ” í° ì°¨ì´ê°€ ì—†ì–´ë³´ì¸ë‹¤.
- ì•ìœ¼ë¡œëŠ” ë…¸ë“œë¥¼ 100ì •ë„ë¡œ ê³ ì •í•  ê²ƒì´ë‹¤

<br>

### ì‹¤í—˜3 : ì€ë‹‰ì¸µì˜ ëª¨ì–‘ì€ ì„±ëŠ¥ê³¼ ì–´ë–¤ ìƒê´€ì´ ìˆì„ê¹Œ?

- 2ê°œ ì€ë‹‰ì¸µì˜ ë…¸ë“œ ë¹„ìœ¨ì„ 1:99 , 2:98 , 3:97 ...... 98:2 , 99:1 ë¡œ ë³€í™”ì‹œì¼œ ë³´ì

![image]({{ site.url }}{{ site.baseurl }}/assets/images/coding/penguins/test_shape.png){: .align-center width="60%"} 

- 2ê°œ ì€ë‹‰ì¸µì˜ ë¹„ìœ¨ì´ í¬ê²Œ ì°¨ì´ë‚˜ëŠ” 1:99, 2:98 ... 98:2, 99:1 ì€ ì„±ëŠ¥ ì ìˆ˜ë„ ë‚®ê³ , ì•ˆì •ê°ë„ ë–¨ì–´ì§€ëŠ” ê²ƒ ê°™ë‹¤.
- ì€ë‹‰ì¸µì˜ ë¹„ìœ¨ì€ ë¹„ìŠ·í•œ ê²ƒì´ ì¢‹ê² ë‹¤.

<br>

### ìµœì  Layer ëª¨ë¸

- ì§€ê¸ˆê¹Œì§€ì˜ ì‹¤í—˜ì„ í† ëŒ€ë¡œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ì

```python
def get_model(nodes=100, lr=0.001):
    random.seed(rand_seed)
    model = tf.keras.Sequential([
                                Dense(nodes, activation='relu', kernel_initializer='he_uniform'),
                                Dense(nodes, activation='relu', kernel_initializer='he_uniform'),
                                Dense(3, activation='softmax'),
                                ])

    model.compile(metrics=['accuracy']
                  , optimizer=tf.keras.optimizers.Adam(learning_rate=lr)
                  , loss='sparse_categorical_crossentropy'
                  )
    
    return model
```

- ì•ìœ¼ë¡œì˜ ì‹¤í—˜ì€ ì•„ë˜ì˜ ëª¨ë¸ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì‹¤í–‰í•  ê²ƒì´ë‹¤

<br>

### ì‹¤í—˜4 : epochs

- epochëŠ” ì¸ê³µì‹ ê²½ë§ì´ ìˆœì „íŒŒì™€ ì—­ì „íŒŒë¥¼ í†µí•˜ì—¬ 1íšŒ í•™ìŠµì´ ì¼ì–´ë‚˜ëŠ” ê³¼ì •ì´ë‹¤.
- ì¸ê³µì‹ ê²½ë§ì—ì„œëŠ” ì—­ì „íŒŒë¥¼ í†µí•˜ì—¬ ëª¨ë¸ì˜ ì—…ë°ì´íŠ¸ê°€ ì¼ì–´ë‚˜ê¸° ë•Œë¬¸ì— epochê°€ ë§ì•„ì§€ë©´ ëª¨ë¸ì´ ì •êµí•´ì§ˆ ê°€ëŠ¥ì„±ì´ ìˆë‹¤.
- epochë¥¼ 1~30 ê¹Œì§€ ì¡°ì •í•˜ë©° ì •í™•ë„ë¥¼ í‰ê°€í•´ë³´ì

![image]({{ site.url }}{{ site.baseurl }}/assets/images/coding/penguins/test_epoch.png){: .align-center width="60%"}  

<br>

### ì‹¤í—˜5 : batch_size

- ê²½ì‚¬í•˜ê°•ë²•ì„ í†µí•˜ì—¬ ì†ì‹¤í•¨ìˆ˜ë¥¼ ê³„ì‚°í•  ë•Œ ì „í†µì ìœ¼ë¡œëŠ” ëª¨ë“  ì…ë ¥ë°ì´í„°ì— ëŒ€í•˜ì—¬ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ì˜€ë‹¤.
- ë°ì´í„°ê°€ ì»¤ì§€ë©´ì„œ ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦¬ê³  ë¹„íš¨ìœ¨ì„±ì´ ë†’ì•„ì ¸ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•, ê³§ ì¼ë¶€ë°ì´í„°ë¥¼ ë½‘ì•„ì„œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì´ ìƒê²¨ë‚¬ë‹¤.
- batch_sizeë¥¼ ì§€ì •í•œë©´ ê·¸ë§Œí¼ ì…ë ¥ë°ì´í„°ë¥¼ ë½‘ì•„ ë¯¸ë‹ˆë°°ì¹˜(Mini-batch) ê²½ì‚¬í•˜ê°•ë²•ì„ ìˆ˜í–‰í•˜ê²Œ ëœë‹¤.
- ì´ë•Œ ì…ë ¥ë°ì´í„° ìˆ˜, batch_size, iterationì€ ë‹¤ìŒê³¼ ê°™ì€ ê´€ê³„ê°€ ì„±ë¦½ëœë‹¤.
- $Nobs. Data = batch\_size \times Iteration(by \; epoch)$
- ì‹¤í—˜ ê²°ê³¼ 80 ì´í•˜ê°€ ì ë‹¹í•œ ê²ƒ ê°™ë‹¤.

![image]({{ site.url }}{{ site.baseurl }}/assets/images/coding/penguins/test_batch_size.png){: .align-center width="60%"}  


<br><br>


## í•™ìŠµë¥  ê³„íš

- ê°€ì¥ ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°€ìš´ë° í•˜ë‚˜ê°€ ë°”ë¡œ í•™ìŠµë¥ (Learning Rate)ì´ë‹¤.
- ì¸ê³µì‹ ê²½ë§ì—ì„œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³„ì‚°ìœ¼ë¡œ ìˆ˜í–‰ëœë‹¤.
- $\theta_{ i,j+1 } = \theta_{ i,j } - \eta { { \delta }\over{ \delta \theta_{ i,j } } }J(\theta)$
  - $\theta_{ i,j+1 }$ : ìƒˆë¡­ê²Œ ìƒì‹ ëœ ê°€ì¤‘ì¹˜
  - $\theta_{ i,j }$ : ê°±ì‹  ì „ ê°€ì¤‘ì¹˜
  - $\eta$ : í•™ìŠµë¥ 
  - ${ { \delta }\over{ \delta \theta_{ i,j } } }J(\theta)$ : í•´ë‹¹ ì§€ì ì—ì„œì˜ ê¸°ìš¸ê¸° 
- ê·¸ë˜ì„œ í•™ìŠµë¥ ì€ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ê³¼ì • ê°€ìš´ë° ê°€ì¤‘ì¹˜ì˜ ë³€í™”í­ì´ë¼ê³  ì´í•´í•  ìˆ˜ ìˆë‹¤.
- í•™ìŠµë¥ ì€ ì§ì—… ì§€ì •í•´ì¤„ ìˆ˜ë„ ìˆì§€ë§Œ ì£¼ë¡œ Step Decay, Cosine Decayì˜ ë‘ê°€ì§€ ë°©ì‹ì´ ìì£¼ ì‚¬ìš©ëœë‹¤.

### í•™ìŠµë¥  Step Decay

- í•™ìŠµë¥  Step DecayëŠ” ì‚¬ìš©ì í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ LearningRateScheduler() ë©”ì¨ë“œì— ë‹´ì•„ ì½œë°±ìœ¼ë¡œ í•™ìŠµê³¼ì •ìœ¼ë¡œ ë„˜ê²¨ì£¼ë©´ ëœë‹¤.

```python
# í•™ìŠµë¥  Step Decayë¥¼ ìœ„í•œ í•¨ìˆ˜
def step_decay(epoch):
    start = 0.2
    drop = 0.5
    epochs_drop = 3
    lr = start * (drop ** np.floor((epoch)/epochs_drop))
    return lr

lr_scheduler = LearningRateScheduler(step_decay, verbose=0)

model = get_model()

model_results = model.fit(x_train, y_train
                          , batch_size              = 16 # None
                          , epochs                  = 10 # 1
                          , verbose                 = 0 # 'auto'
                          , callbacks               = [lr_scheduler] # None
                          )

model_accuracy = print_accuracy(model, verbose=1)

# ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
fig, ax = plt.subplots( figsize=(5, 4) )
fig.suptitle('Step Decay', fontsize = 22, fontweight = 'bold', y = 1)

sns.lineplot(x=range(1,11), y=model_results.history['lr'], color='r', label='Learning Rate')

plt.xlabel('epoch', fontsize = 14)
plt.ylabel('Learning Rate', fontsize = 14)
plt.show()
```

- í•™ìŠµë¥ ì´ epochê°€ ì§„í–‰ë¨ì— ë”°ë¼ ë§Œë“  í•¨ìˆ˜ì— ë”°ë¼ì„œ ê°ì†Œí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.
- Train Accuracy : 0.996, Test Accuracy : 0.986

![image]({{ site.url }}{{ site.baseurl }}/assets/images/coding/penguins/step_decay.png){: .align-center width="60%"} 

<br>

### í•™ìŠµë¥  Cosine Decay

- í•™ìŠµë¥  Cosine Decay ëŠ” ì½”ì‚¬ì¸ì˜ í˜•íƒœë¡œ í•™ìŠµë¥ ì„ ê°ì†Œì‹œí‚¨ë‹¤.
- Step Decayì²˜ëŸ¼ ê¸‰ê²©í•˜ê²Œ í•™ìŠµë¥ ì´ ë³€í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì½”ì‚¬ì¸ ê³¡ì„ ì„ ê·¸ë¦¬ë©° í•™ìŠµë¥ ì´ ë¯¸ì„¸í•˜ê²Œ ê°ì†Œí•˜ë„ë¡ í•  ìˆ˜ ìˆë‹¤.
- Train Accuracy : 0.913, Test Accuracy : 0.971

```python
first_decay_steps = 1000
initial_learning_rate = 0.2

# Cosine Learning Rate Decay
cos_decay = (
  tf.keras.experimental.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps)
  )

model = get_model(lr=cos_decay)

model_results = model.fit(x_train, y_train
                          , batch_size              = 16 # None
                          , epochs                  = 10 # 1
                          , verbose                 = 0 # 'auto'
                          )

model_accuracy = print_accuracy(model, verbose=1)
```


<br><br>


## ê³¼ì í•© ë°©ì§€

### ê°€ì¤‘ì¹˜ ê°ì†Œ(Weight Decay)

- ê°€ì¤‘ì¹˜ ê°ì†Œë¥¼ ìœ„í•œ ì •ê·œí™”ëŠ” L1 Regularization(LASSO), L2 Regularization(Ridge)ì˜ ë‘ ê°€ì§€ê°€ ìˆë‹¤.
- kernel_regularizer, activity_regularizerì˜ ì°¨ì´ëŠ” ì•„ì§ ëª¨ë¥´ê² ë‹¤.
- Train Accuracy : 0.993, Test Accuracy : 0.957
- ì ìˆ˜ê°€ ì˜ ë‚˜ì˜¤ë‹ˆ ì¼ë‹¨ ì‚¬ìš©í•˜ì...ğŸ˜³

```python
first_decay_steps = 1000
initial_learning_rate = 0.2

# Cosine Learning Rate Decay
cos_decay = (
  tf.keras.experimental.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps)
  )

model = tf.keras.Sequential([
                            Dense(100
                                  , activation='relu'
                                  , kernel_initializer='he_uniform'
                                  , kernel_regularizer=L1(0.01)
                                  , activity_regularizer=L2(0.01)
                                  ),
                            Dense(100
                                  , activation='relu'
                                  , kernel_initializer='he_uniform'
                                  , kernel_regularizer=L2(0.01)
                                  , activity_regularizer=L1(0.01)
                                  ),
                            Dense(3, activation='softmax'),
                            ])

model.compile(metrics=['accuracy']
              , optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)
              , loss='sparse_categorical_crossentropy'
              )

model_results = model.fit(x_train, y_train
                          , batch_size              = 16 # None
                          , epochs                  = 10 # 1
                          , verbose                 = 0 # 'auto'
                          )

model_accuracy = print_accuracy(model, verbose=1)
```

<br>

### Dropout

- ì€ë‹‰ì¸µì˜ ì¼ë¶€ ë…¸ë“œë¥¼ í•™ìŠµì— ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ë°©ë²•ì´ë‹¤.
- ì€ë‹‰ì¸µ ë’¤ì— Dropout(0.5) ë©”ì¨ë“œë¥¼ ì¶”ê°€í•˜ì—¬ ì„¤ì •í•  ìˆ˜ ìˆë‹¤.
- Train Accuracy : 0.975, Test Accuracy : 0.957

```python
first_decay_steps = 1000
initial_learning_rate = 0.2

# Cosine Learning Rate Decay
cos_decay = (
  tf.keras.experimental.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps)
  )

model = tf.keras.Sequential([
                            Dense(100
                                  , activation='relu'
                                  , kernel_initializer='he_uniform'
                                  , kernel_regularizer=L1(0.01)
                                  , activity_regularizer=L2(0.01)
                                  ),
                            Dense(100
                                  , activation='relu'
                                  , kernel_initializer='he_uniform'
                                  , kernel_regularizer=L2(0.01)
                                  , activity_regularizer=L1(0.01)
                                  ),
                            Dropout(0.5),
                            Dense(3, activation='softmax'),
                            ])

model.compile(metrics=['accuracy']
              , optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)
              , loss='sparse_categorical_crossentropy'
              )

model_results = model.fit(x_train, y_train
                          , batch_size              = 16 # None
                          , epochs                  = 10 # 1
                          , verbose                 = 0 # 'auto'
                          )

model_accuracy = print_accuracy(model, verbose=1)
```

<br>

### Early Stopping

- ê³¼ì •í•©ì´ ì‹¬í•´ì§€ê¸° ì „ì— í•™ìŠµì„ ë©ˆì¶”ë„ë¡ ì„¤ì •

```python
first_decay_steps = 1000
initial_learning_rate = 0.2

# Cosine Learning Rate Decay
cos_decay = (
  tf.keras.experimental.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps)
  )

# Early Stopping
early_stop = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=3, verbose=1)

model = tf.keras.Sequential([
                            Dense(100
                                  , activation='relu'
                                  , kernel_initializer='he_uniform'
                                  , kernel_regularizer=L1(0.01)
                                  , activity_regularizer=L2(0.01)
                                  ),
                            Dense(100
                                  , activation='relu'
                                  , kernel_initializer='he_uniform'
                                  , kernel_regularizer=L2(0.01)
                                  , activity_regularizer=L1(0.01)
                                  ),
                            Dropout(0.5),
                            Dense(3, activation='softmax'),
                            ])

model.compile(metrics=['accuracy']
              , optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)
              , loss='sparse_categorical_crossentropy'
              )

model_results = model.fit(x_train, y_train, validation_data=(x_test, y_test)
                          , batch_size        = 16 # None
                          , epochs            = 20 # 1
                          , verbose           = 1 # 'auto'
                          , callbacks         = [early_stop]
                          )

model_accuracy = print_accuracy(model, verbose=1)
```

- ë‹¤ìŒê³¼ ê°™ì´ 11ë²ˆì§¸ epochì—ì„œ ê³¼ì í•©ì´ ë°œìƒí•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆë‹¤
- Train Accuracy : 0.982, Test Accuracy : 0.957

```
Epoch 1/20
18/18 [==============================] - 1s 17ms/step - loss: 6.1275 - accuracy: 0.6509 - val_loss: 5.5955 - val_accuracy: 0.7681
Epoch 2/20
18/18 [==============================] - 0s 6ms/step - loss: 5.4336 - accuracy: 0.7891 - val_loss: 5.1008 - val_accuracy: 0.8551
Epoch 3/20
18/18 [==============================] - 0s 6ms/step - loss: 4.9484 - accuracy: 0.8691 - val_loss: 4.7225 - val_accuracy: 0.8551
Epoch 4/20
18/18 [==============================] - 0s 5ms/step - loss: 4.5982 - accuracy: 0.8764 - val_loss: 4.4008 - val_accuracy: 0.8986
Epoch 5/20
18/18 [==============================] - 0s 5ms/step - loss: 4.3068 - accuracy: 0.8945 - val_loss: 4.1185 - val_accuracy: 0.9420
Epoch 6/20
18/18 [==============================] - 0s 5ms/step - loss: 4.0102 - accuracy: 0.9273 - val_loss: 3.8753 - val_accuracy: 0.9420
Epoch 7/20
18/18 [==============================] - 0s 6ms/step - loss: 3.7918 - accuracy: 0.9564 - val_loss: 3.6620 - val_accuracy: 0.9420
Epoch 8/20
18/18 [==============================] - 0s 6ms/step - loss: 3.5845 - accuracy: 0.9600 - val_loss: 3.4688 - val_accuracy: 0.9565
Epoch 9/20
18/18 [==============================] - 0s 6ms/step - loss: 3.4150 - accuracy: 0.9564 - val_loss: 3.2923 - val_accuracy: 0.9565
Epoch 10/20
18/18 [==============================] - 0s 6ms/step - loss: 3.2399 - accuracy: 0.9673 - val_loss: 3.1362 - val_accuracy: 0.9565
Epoch 11/20
18/18 [==============================] - 0s 7ms/step - loss: 3.0894 - accuracy: 0.9709 - val_loss: 2.9861 - val_accuracy: 0.9565
Epoch 11: early stopping
```


<br><br>


## í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

- í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ í•˜ë‚˜ì”© ì‹¤í—˜í•´ë³´ëŠ” ê²ƒì€ ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦°ë‹¤.
- ê·¸ë˜ì„œ Grid Search, Random Search ê°€ ìì£¼ ì‚¬ìš©ëœë‹¤.
- Keras Tuner ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ”ë° ë„ì›€ì´ ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë‹¤.
- ë¨¼ì € í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìœ„í•˜ì—¬ ì§€ê¸ˆê¹Œì§€ì˜ ì‹¤í—˜ì„ í†µí•˜ì—¬ ì ì ˆí•œ ëª¨ë¸ì„ ë§Œë“œëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì

```python
def get_model(nodes=100, lr=0.001, kernel_l1=0.01, kernel_l2=0.01, activity_l1=0.01, activity_l2=0.01, dropout=0.5):
    random.seed(rand_seed)
    model = tf.keras.Sequential([
                        Dense(nodes
                                , activation='relu'
                                , kernel_initializer='he_uniform'
                                , kernel_regularizer=L1(kernel_l1)
                                , activity_regularizer=L2(activity_l2)
                                , input_dim=x_train.shape[1]
                                ),
                        Dense(nodes
                                , activation='relu'
                                , kernel_initializer='he_uniform'
                                , kernel_regularizer=L2(kernel_l2)
                                , activity_regularizer=L1(activity_l1)
                                ),
                        Dropout(dropout),
                        Dense(3, activation='softmax'),
                                ])

    model.compile(metrics=['accuracy']
                  , optimizer=keras.optimizers.Adam(learning_rate=lr)
                  , loss='sparse_categorical_crossentropy'
                  )
    
    return model
```

<br>

### Random Search

- Random Search ëŠ” ì§€ì •ëœ ë²”ìœ„ ì•ˆì—ì„œ ì§€ì •ëœ íšŸìˆ˜ë§Œí¼ ëœë¤í•œ ì¡°í•©ìœ¼ë¡œ í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤.
- ì²˜ìŒ ì ‘í•˜ëŠ” ë°ì´í„°, ë„“ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ì— ëŒ€í•˜ì—¬ í•™ìŠµí•  ë•Œ ì‚¬ìš©ëœë‹¤.

```python
verbose = 0
model = KerasClassifier(model=get_model, verbose=verbose)

# Make Params
params = {
    "optimizer__learning_rate"          : uniform(loc=0.001, scale=0.01),
    "batch_size"                        : range(2, 16),
    "epochs"                            : range(6, 15),
}

randomized_search = RandomizedSearchCV(
                    model,
                    param_distributions   = params,
                    scoring               = "accuracy",
                    n_iter                = 10,
                    cv                    = 3,
                    verbose               = verbose,
                    random_state          = rand_seed,
                    )

randomized_search.fit(x_train, y_train, validation_data=(x_test, y_test)
                      , verbose     = verbose
                      , epochs      = 10
                      , batch_size  = 4
                      , callbacks=[early_stop]
                      )
```

```
ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° : {'batch_size': 8, 'epochs': 8, 'optimizer__learning_rate': 0.005731505218679044}
ìµœì  ì •í™•ë„         : 0.9964

Best: 0.9963768115942028 using {'batch_size': 8, 'epochs': 8, 'optimizer__learning_rate': 0.005731505218679044}
Means: 0.974 / {'batch_size': 4, 'epochs': 13, 'optimizer__learning_rate': 0.007837875830710724}
Means: 0.989 / {'batch_size': 2, 'epochs': 6, 'optimizer__learning_rate': 0.008771304344664}
Means: 0.971 / {'batch_size': 14, 'epochs': 10, 'optimizer__learning_rate': 0.0048673606674801756}
Means: 0.982 / {'batch_size': 2, 'epochs': 11, 'optimizer__learning_rate': 0.0038254051509577884}
Means: 0.996 / {'batch_size': 8, 'epochs': 8, 'optimizer__learning_rate': 0.005731505218679044}
Means: 0.978 / {'batch_size': 7, 'epochs': 6, 'optimizer__learning_rate': 0.0045319254935582785}
Means: 0.985 / {'batch_size': 11, 'epochs': 11, 'optimizer__learning_rate': 0.004820023120561529}
Means: 0.982 / {'batch_size': 6, 'epochs': 12, 'optimizer__learning_rate': 0.003425228681016678}
Means: 0.978 / {'batch_size': 4, 'epochs': 12, 'optimizer__learning_rate': 0.007947590052146253}
Means: 0.993 / {'batch_size': 10, 'epochs': 8, 'optimizer__learning_rate': 0.007231445127707466}
```

|    |   mean_test_score |   param_batch_size |   param_epochs |   param_optimizer__learning_rate |
|---:|------------------:|-------------------:|---------------:|---------------------------------:|
|  4 |          0.996377 |                  8 |              8 |                       0.00573151 |
|  9 |          0.992714 |                 10 |              8 |                       0.00723145 |
|  1 |          0.989051 |                  2 |              6 |                       0.0087713  |
|  6 |          0.985428 |                 11 |             11 |                       0.00482002 |
|  3 |          0.981725 |                  2 |             11 |                       0.00382541 |
|  7 |          0.981725 |                  6 |             12 |                       0.00342523 |
|  8 |          0.978141 |                  4 |             12 |                       0.00794759 |
|  5 |          0.978102 |                  7 |              6 |                       0.00453193 |
|  0 |          0.974439 |                  4 |             13 |                       0.00783788 |
|  2 |          0.970815 |                 14 |             10 |                       0.00486736 |

```
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_93 (Dense)            (None, 100)               700       
                                                                 
 dense_94 (Dense)            (None, 100)               10100     
                                                                 
 dropout_31 (Dropout)        (None, 100)               0         
                                                                 
 dense_95 (Dense)            (None, 3)                 303       
                                                                 
=================================================================
Total params: 11,103
Trainable params: 11,103
Non-trainable params: 0
```

![image]({{ site.url }}{{ site.baseurl }}/assets/images/coding/penguins/randomized_search_CM.png){: .align-center width="60%"}  

<br>

### Grid Search

- Grid SearchëŠ” ì§€ì •ëœ ë²”ìœ„ì˜ ëª¨ë“  ì¡°í•©ì— ëŒ€í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•œë‹¤.
- í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ê°€ ë„“ìœ¼ë©´ ì¡°í•©ì´ ë§ì•„ì§€ê³ , í•™ìŠµì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦°ë‹¤.
- ê·¸ë˜ì„œ ì¢ì€ ë²”ìœ„ì˜ ë¯¸ì„¸í•œ ì¡°ì •ì´ í•„ìš”í• ë•Œ Grid Searchê°€ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.

```python
verbose = 0
model = KerasClassifier(model=get_model, verbose=verbose)

# Make Params
params = {
    "optimizer__learning_rate"          : [0.001, 0.005],
    "batch_size"                        : [10, 16],
    "epochs"                            : [10, 12],
}

grid_search = GridSearchCV(
              model,
              param_grid            = params,
              scoring               = "accuracy",
              cv                    = 3,
              verbose               = verbose,
              )

grid_search.fit(x_train, y_train, validation_data=(x_test, y_test)
                , verbose     = verbose
                , epochs      = 10
                , batch_size  = 4
                , callbacks=[early_stop]
                )
```

```
ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° : {'batch_size': 16, 'epochs': 12, 'optimizer__learning_rate': 0.001}
ìµœì  ì •í™•ë„         : 0.9927

Best: 0.9927138079311991 using {'batch_size': 16, 'epochs': 12, 'optimizer__learning_rate': 0.001}
Means: 0.978 / {'batch_size': 10, 'epochs': 10, 'optimizer__learning_rate': 0.001}
Means: 0.978 / {'batch_size': 10, 'epochs': 10, 'optimizer__learning_rate': 0.005}
Means: 0.978 / {'batch_size': 10, 'epochs': 12, 'optimizer__learning_rate': 0.001}
Means: 0.985 / {'batch_size': 10, 'epochs': 12, 'optimizer__learning_rate': 0.005}
Means: 0.985 / {'batch_size': 16, 'epochs': 10, 'optimizer__learning_rate': 0.001}
Means: 0.982 / {'batch_size': 16, 'epochs': 10, 'optimizer__learning_rate': 0.005}
Means: 0.993 / {'batch_size': 16, 'epochs': 12, 'optimizer__learning_rate': 0.001}
Means: 0.974 / {'batch_size': 16, 'epochs': 12, 'optimizer__learning_rate': 0.005}
```

|    |   mean_test_score |   param_batch_size |   param_epochs |   param_optimizer__learning_rate |
|---:|------------------:|-------------------:|---------------:|---------------------------------:|
|  6 |          0.992714 |                 16 |             12 |                            0.001 |
|  3 |          0.985467 |                 10 |             12 |                            0.005 |
|  4 |          0.985428 |                 16 |             10 |                            0.001 |
|  5 |          0.981725 |                 16 |             10 |                            0.005 |
|  0 |          0.978102 |                 10 |             10 |                            0.001 |
|  2 |          0.978102 |                 10 |             12 |                            0.001 |
|  1 |          0.978062 |                 10 |             10 |                            0.005 |
|  7 |          0.974478 |                 16 |             12 |                            0.005 |

```
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_171 (Dense)           (None, 100)               700       
                                                                 
 dense_172 (Dense)           (None, 100)               10100     
                                                                 
 dropout_57 (Dropout)        (None, 100)               0         
                                                                 
 dense_173 (Dense)           (None, 3)                 303       
                                                                 
=================================================================
Total params: 11,103
Trainable params: 11,103
Non-trainable params: 0
```

![image]({{ site.url }}{{ site.baseurl }}/assets/images/coding/penguins/grid_search_CM.png){: .align-center width="60%"}  

<br>

### Keras Tuner

- Keras ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•˜ëŠ”ë° ë„ì›€ì´ ë˜ëŠ” Keras Tuner ë¥¼ ì œê³µí•œë‹¤.
- ìì„¸í•œ ë‚´ìš©ì€ [Keras Tuner ê³µì‹ë¬¸ì„œ](https://www.tensorflow.org/tutorials/keras/keras_tuner?hl=ko) ì°¸ì¡°


```python
def kt_model(hp):
    random.seed(rand_seed)

    # Cosine Learning Rate Decay
    cos_decay = (
      tf.keras.experimental.CosineDecayRestarts(
          initial_learning_rate,
          first_decay_steps)
      )

    hp_units = hp.Int('units', min_value = 16, max_value = 256, step = 16)
    hp_dropout = hp.Choice('rate', values = [2e-1, 4e-1, 6e-1])

    model = tf.keras.Sequential([
                        Dense(units = hp_units
                                , activation='relu'
                                , kernel_initializer='he_uniform'
                                , kernel_regularizer=L1(0.01)
                                , activity_regularizer=L2(0.01)
                                , input_dim=x_train.shape[1]
                                ),
                        Dense(units = hp_units
                                , activation='relu'
                                , kernel_initializer='he_uniform'
                                , kernel_regularizer=L2(0.01)
                                , activity_regularizer=L1(0.01)
                                ),
                        Dropout(rate=hp_dropout),
                        Dense(3, activation='softmax'),
                                ])

    model.compile(metrics=['accuracy']
                  , optimizer=keras.optimizers.Adam(learning_rate=cos_decay)
                  , loss='sparse_categorical_crossentropy'
                  )
    
    return model

tuner = kt.Hyperband(kt_model,
                     objective = 'val_accuracy',
                     max_epochs = 10,
                     factor = 3,
                     directory = 'my_dir',
                     project_name = 'intro_to_kt')

class ClearTrainingOutput(tf.keras.callbacks.Callback):
  def on_train_end(*args, **kwargs):
    IPython.display.clear_output(wait = True)

tuner.search(x_train, y_train, validation_data = (x_test, y_test)
             , epochs = 10
             , callbacks = [ClearTrainingOutput()]
             )

best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]
```


```
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_6 (Dense)             (None, 100)               700       
                                                                 
 dense_7 (Dense)             (None, 100)               10100     
                                                                 
 dropout_2 (Dropout)         (None, 100)               0         
                                                                 
 dense_8 (Dense)             (None, 3)                 303       
                                                                 
=================================================================
Total params: 11,103
Trainable params: 11,103
Non-trainable params: 0
```

![image]({{ site.url }}{{ site.baseurl }}/assets/images/coding/penguins/keras_tuner_CM.png){: .align-center width="60%"} 


<br><br>


## ê²°ë¡ 

>- ì¸ê³µì‹ ê²½ë§ìœ¼ë¡œ ê¸°ì¤€ëª¨ë¸ì¸ ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨ë¸ ë§Œí¼ì€ ë‚˜ì˜¨ê²ƒ ê°™ë‹¤.
>- ë°ì´í„°ê°€ ì‘ê³ , ë‹¨ìˆœí•˜ë©´ ê·¸ëƒ¥ ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨ë¸ë¡œë„ ì¶©ë¶„í•˜ë‹¤.
>- ë³µì¡í•œ ë°ì´í„°ë¡œ ì¸ê³µì‹ ê²½ë§ì„ êµ¬ì„±í•˜ë©´ íš¨ê³¼ê°€ í´ ê²ƒ ê°™ë‹¤.  
>- ë‹­ì¡ëŠ”ë° ì†Œì¡ëŠ” ì¹¼ ì“°ì§€ë§ì

<br>

![image](https://leeyeonjun85.github.io/home/assets/images/etc/kalkal.jpg){: .align-center width="60%"}  




<br>
<br>
<br>
<br>


<center>
<h1>ëê¹Œì§€ ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤ğŸ˜‰</h1>
</center>


<br>
<br>
<br>
<br>


<!-- 


{: .notice}
{: .notice--primary}
{: .notice--info}
{: .notice--warning}
{: .notice--danger}
{: .notice--success}

 -->





