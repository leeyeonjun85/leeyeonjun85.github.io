---

title: "AI ë¶€íŠ¸ìº í”„ 10ì£¼ : Natural Language Processing"

excerpt: "ì½”ë“œìŠ¤í…Œì´ì¸ ì™€ í•¨ê»˜í•˜ëŠ” 'AI ë¶€íŠ¸ìº í”„' 10ì£¼ì°¨ íšŒê³ "

categories:
  - AIB Log

tags:
  - ê°œë°œì¼ì§€
  - ì½”ë”©
  - AI ë¶€íŠ¸ìº í”„
  - ì½”ë“œìŠ¤í…Œì´ì¸ 

header:
  teaser: /assets/images/aib/codestates-ci.png

last_modified_at: 2023-02-27

---




<br>
<br>
<br>
<br>


![image](https://leeyeonjun85.github.io/home/assets/images/etc/writing-705667_1920.jpg){: .align-center width="70%"}  


<br>
<br>
<br>
<br>





# ì½”ë“œìŠ¤í…Œì´ì¸ ì™€ í•¨ê»˜í•˜ëŠ” 'AI ë¶€íŠ¸ìº í”„' 10ì£¼ì°¨







<br><br><br><br>

## ì£¼ê°„íšŒê³ 
{: style="text-align: center;"}

<br><br><br><br>




### ë” ê³µë¶€ê°€ í•„ìš”í•œ ë¶€ë¶„
> Functional API, Subclassing API, ì „ì´í•™ìŠµ, Fine-tuning

### ì£¼ê°„ íšŒê³ 
- ì‚¬ì‹¤(Fact)  
ë”¥ëŸ¬ë‹ë¶„ì•¼ì˜ ìì—°ì–´ì²˜ë¦¬ì— ëŒ€í•˜ì—¬ í•™ìŠµí•˜ì˜€ë‹¤.

- ëŠë‚Œ(Feeling)  
ìš”ì¦˜ ê°€ì¥ í•«í•œ ì±—GPTë¥¼ ë§Œë“¤ìˆ˜ìˆë‹¤ëŠ” ìƒê°ì— ë¶€í‘¼ ê¸°ëŒ€ë¥¼ ì•ˆê³  ì‹œì‘í–ˆì§€ë§Œ, ì•„ì§ ë‚´ê°€ ë„ì „í•˜ê¸°ì—ëŠ” ë”¥ëŸ¬ë‹ì— ëŒ€í•œ ì´í•´ê°€ ë§ì´ ë¶€ì¡±í•¨ì„ ëŠê¼ˆë‹¤. í•˜ì§€ë§Œ ë§ˆì¹˜ ë“±ì‚°ì„ í•  ë•Œ <mark>ì € ë©€ë¦¬ ì •ìƒì´ ë³´ì´ëŠ” ëŠë‚Œ</mark>ì´ ë“¤ì–´ ìƒˆë¡œìš´ ì˜ì§€ê°€ ìƒê¸°ê¸°ë„ í•˜ì˜€ë‹¤.

- êµí›ˆ(Finding)  
ëª¨ë¸ì„ ì´í•´í•˜ê¸° ìœ„í•˜ì—¬ íƒ„íƒ„í•œ ê¸°ë³¸ê¸°(ì „ì²˜ë¦¬, ë²¡í„°í™”, Functional API, Subclassing API)ê°€ í•„ìš”í•˜ë‹¤.

- í–¥í›„ í–‰ë™(Future action)  
Functional API, Subclassing APIë¥¼ ì—°ìŠµí•˜ê³ , ëª¨ë¸ì„ í•˜ë‚˜ì”© ëœ¯ì–´ë³´ë©´ì„œ ì´í•´í•´ë³´ì





<br><br><br><br>

## N321 : Natural Language Processing
{: style="text-align: center;"}

<br><br><br><br>




### ğŸ† í•™ìŠµ ëª©í‘œ
#### Level 1 : Lecture Note ì— ìˆëŠ” ì£¼ìš” ê°œë…ì„ ì„¤ëª…í•  ìˆ˜ ìˆìœ¼ë©° ì˜ˆì œ ì½”ë“œë¥¼ ì´í•´í•˜ê³  ì¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ìì—°ì–´ì²˜ë¦¬ë¥¼ í†µí•´ í•  ìˆ˜ ìˆëŠ” Task ì—ëŠ” ì–´ë–¤ ê²ƒì´ ìˆëŠ”ì§€ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ë¶ˆìš©ì–´(Stop words), ì–´ê°„ ì¶”ì¶œ(Stemming)ê³¼ í‘œì œì–´ ì¶”ì¶œ(Lemmatization) ë“±ì— ëŒ€í•´ ì„¤ëª…í•  ìˆ˜ ìˆê³  ì´ë¥¼ ì ìš©í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- Bag-of-words ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆìœ¼ë©° ì‚¬ì´í‚·ëŸ° ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì´ë¥¼ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ê°œë…ê³¼ ì½”ë”© Quizë¥¼ í†µí•´ì„œ ìœ„ í•™ìŠµ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ì˜€ëŠ”ì§€ ì ê²€í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Level 2 : ğŸ” Reference ë‚´ìš©ì„ ì´í•´í•˜ê³ , ì´ë¥¼ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì— ì ìš©í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- N-gramì˜ ê°œë…ì— ëŒ€í•´ ì´í•´í•˜ê³  Bag-of-Words ì— ì ìš©í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- Spacy ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë‹¤ë¥¸ ê¸°ëŠ¥ì„ í…ìŠ¤íŠ¸ì— ì ìš©í•˜ì—¬ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- Discussion ì— ìˆëŠ” ì½”ë”© ê³¼ì œë¥¼ í†µí•´ì„œ ìœ„ í•™ìŠµ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ì˜€ëŠ”ì§€ ì ê²€í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Level 3 : ğŸ”¥ Reference ë‚´ìš©ì„ ì„¤ëª…í•  ìˆ˜ ìˆê³ , í•´ë‹¹ ë‚´ìš©ì„ ì½”ë“œë¡œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- LSA(ì ì¬ ì˜ë¯¸ ë¶„ì„)ì— ëŒ€í•´ ì´í•´í•˜ê³  ì½”ë“œë¡œ ì ìš©í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.



### ì˜¤ëŠ˜ì˜ í‚¤ì›Œë“œ
- ìì—°ì–´ì²˜ë¦¬
- í…ìŠ¤íŠ¸ì „ì²˜ë¦¬
- ë²¡í„°í™”
- TF, TF-IDF
- ì½”ì‚¬ì¸ìœ ì‚¬ë„



### ê³µë¶€í•œ ë‚´ìš©
#### ìì—°ì–´ì²˜ë¦¬ë¡œ í•  ìˆ˜ ìˆëŠ” ì¼ë“¤
> ìì—°ì–´ ì´í•´, ìì—°ì–´ ìƒì„±, ê¸°íƒ€  

##### ìì—°ì–´ ì´í•´(NLU, Natural Language Understanding)  
- ë¶„ë¥˜(Classification)
  - ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ë¥˜, ê°ì„± ë¶„ì„(Positive/Negative)
- ìì—°ì–´ ì¶”ë¡ (NLI, Natural Langauge Inference)
- ê¸°ê³„ ë…í•´(MRC, Machine Reading Comprehension), ì§ˆì˜ ì‘ë‹µ(QA, Question&Answering)
- í’ˆì‚¬ íƒœê¹…(POS tagging), ê°œì²´ëª… ì¸ì‹(Named Entity Recognition) 

##### ìì—°ì–´ ìƒì„±(NLG, Natural Language Generation)  
- í…ìŠ¤íŠ¸ ìƒì„±

##### ê¸°íƒ€  
- ìì—°ì–´ ì´í•´, ìì—°ì–´ ìƒì„± í˜¼í•©
  - ê¸°ê³„ ë²ˆì—­(Machine Translation)
  - ìš”ì•½(Summerization)
  - ì±—ë´‡(Chatbot)
- TTS(Text to Speech) : í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ì½ê¸°
- STT(Speech to Text) : ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ì“°ê¸°
- Image Captioning : ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•˜ëŠ” ë¬¸ì¥ ìƒì„±


#### í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬(Text Preprocessing)
> í•„ìš”ì—†ëŠ” ê±° ì‚­ì œí•˜ê³  í† í°í™”
- ë‚´ì¥ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•œ ì „ì²˜ë¦¬ (lower, replace, ...)
- ì •ê·œ í‘œí˜„ì‹(Regular expression, Regex)
- ë¶ˆìš©ì–´(Stop words) ì²˜ë¦¬
- í†µê³„ì  íŠ¸ë¦¬ë°(Trimming)
- ì–´ê°„ ì¶”ì¶œ(Stemming) í˜¹ì€ í‘œì œì–´ ì¶”ì¶œ(Lemmatization)


#### ë²¡í„°í™”(Vectorize)  
> íšŸìˆ˜ ê¸°ë°˜, ë¶„í¬ ê¸°ë°˜

##### ë“±ì¥ íšŸìˆ˜ ê¸°ë°˜ì˜ ë‹¨ì–´ í‘œí˜„(Count-based Representation)
- Bag-of-Words (CounterVectorizer) : TF(Term Frequency)

![image](https://dudeperf3ct.github.io/images/lstm_and_gru/bag-of-words.png){: .align-center width="60%"} 

- TF-IDF(Term Frequency-Inverse Document Frequency)
  - ë‹¤ë¥¸ ë¬¸ì„œì— ë“±ì¥í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´, ë‹¤ë¥¸ ë¬¸ì„œì—ëŠ” ì˜ ë“±ì¥í•˜ì§€ ì•ŠëŠ”ë° í•´ë‹¹ ë¬¸ì„œì—ë§Œ ì£¼ìš”í•˜ê²Œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´, ê³§ í•´ë‹¹ ë¬¸ì„œë¥¼ íŠ¹ì§•ì§“ëŠ” ë‹¨ì–´ì— ê°€ì¤‘ì¹˜ë¥¼ ë‘ëŠ” ë°©ì‹
  - ì‹œê·¸ë‹ˆì³(Signature)ì— ì§‘ì¤‘í•˜ê¸°
  - $\text{TF-IDF(w)} = \text{TF(w)} \times \text{IDF(w)}$
  - $\text{TF(w)} = \text{íŠ¹ì • ë¬¸ì„œ ë‚´ ë‹¨ì–´ wì˜ ìˆ˜}$
  - $\text{IDF(w)} = \log \bigg(\frac{\text{ë¶„ë¥˜ ëŒ€ìƒì´ ë˜ëŠ” ëª¨ë“  ë¬¸ì„œì˜ ìˆ˜}}{\text{ë‹¨ì–´ wê°€ ë“¤ì–´ìˆëŠ” ë¬¸ì„œì˜ ìˆ˜}}\bigg)$
  - $\text{IDF(w)} = \log \bigg(\frac{n}{1 + df(w)}\bigg)$
    - $n$ : ë¶„ë¥˜ ëŒ€ìƒì´ ë˜ëŠ” ëª¨ë“  ë¬¸ì„œì˜ ìˆ˜
    - $df(w)$ : ë‹¨ì–´ $w$ê°€ ë“¤ì–´ìˆëŠ” ë¬¸ì„œì˜ ìˆ˜
    - 1ì„ ë”í•˜ëŠ” ì´ìœ ëŠ” Division by Zero Errorë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•˜ì—¬



##### ë¶„í¬ ê¸°ë°˜ì˜ ë‹¨ì–´ í‘œí˜„(Distributed Representation)
- Word2Vec
- GloVe
- fastText


#### ì½”ì‚¬ì¸ ìœ ì‚¬ë„(Consine Similarity)  
- $\Large \text{cosine similarity} = cos (\theta)=\frac{Aâ‹…B}{\Vert A\Vert \Vert B \Vert}$

![image](https://images.deepai.org/glossary-terms/cosine-similarity-1007790.jpg){: .align-center width="60%"}  

- ìœ„ ê·¸ë¦¼ì—ì„œ ë‘ ë²¡í„°(ë¬¸ì„œ)ê°€ ì´ë£¨ëŠ” ê°ì´
  - ì™¼ìª½ : $0^\circ$ = 1
  - ê°€ìš´ë° : $90^\circ$ = 0
  - ì˜¤ë¥¸ìª½ : $180^\circ$ = -1


<br><br><br><br>

## N322 : Distributed Representation
{: style="text-align: center;"}

<br><br><br><br>




### ğŸ† í•™ìŠµ ëª©í‘œ
#### Level 1 : Lecture Note ì— ìˆëŠ” ì£¼ìš” ê°œë…ì„ ì„¤ëª…í•  ìˆ˜ ìˆìœ¼ë©° ì˜ˆì œ ì½”ë“œë¥¼ ì´í•´í•˜ê³  ì¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì„ë² ë”©(Embedding)ì˜ ê°œë…ê³¼ ì›-í•« ì¸ì½”ë”©ê³¼ ë¹„êµë˜ëŠ” ì¥ì ì— ëŒ€í•´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- Word2Vecì˜ ë‘ ë°©ë²•(CBoW, Skip-gram)ì˜ ì°¨ì´ì™€ Word2Vecë¡œ ì„ë² ë”©í•œ ë‹¨ì–´ ë²¡í„°ì˜ íŠ¹ì§•ì— ëŒ€í•´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ê°œë…ê³¼ ì½”ë”© Quizë¥¼ í†µí•´ì„œ ìœ„ í•™ìŠµ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ì˜€ëŠ”ì§€ ì ê²€í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Level 2 : ğŸ” Reference ë‚´ìš©ì„ ì´í•´í•˜ë©° ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- fastTextì—ì„œ ì ìš©ëœ ì² ì ë‹¨ìœ„ ì„ë² ë”©(Character-Level Embedding) ë°©ë²•ì˜ ì¥ì ì— ëŒ€í•´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- Discussionì„ í†µí•´ì„œ ìœ„ í•™ìŠµ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ì˜€ëŠ”ì§€ ì ê²€í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Level 3 : ğŸ”¥ Reference ë‚´ìš©ì„ ì„¤ëª…í•  ìˆ˜ ìˆê³ , í•´ë‹¹ ë‚´ìš©ì„ ì½”ë“œë¡œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì„ë² ë”©(Embedding)ì´ ë‹¤ë¥¸ ë„ë©”ì¸ì—ì„œëŠ” ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ” ì§€ í­ë„“ê²Œ ì´í•´í•˜ë©° ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



### ì˜¤ëŠ˜ì˜ í‚¤ì›Œë“œ
- ì„ë² ë”©
- Word2Vec
- gensim



### ê³µë¶€í•œ ë‚´ìš©  
> í…ìŠ¤íŠ¸ ë²¡í„°í™” : ë“±ì¥ íšŸìˆ˜ ê¸°ë°˜, ë¶„í¬ ê¸°ë°˜  

#### ë¶„ì‚° í‘œí˜„(Distributed Representation)
- ë“±ì¥ íšŸìˆ˜ ê¸°ë°˜ í‘œí˜„ vs. ë¶„ì‚° í‘œí˜„
- ë¶„ì‚° í‘œí˜„ì´ ì„¤ë“ë ¥ì„ ê°€ì§€ëŠ” ì´ìœ ?
  - ë¶„í¬ ê°€ì„¤(Distribution hypothesis) ë•Œë¬¸
  - ë¹„ìŠ·í•œ ìœ„ì¹˜ì—ì„œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ì€ ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤
- ë¶„í¬ ê°€ì„¤ì— ê¸°ë°˜í•˜ì—¬ ì£¼ë³€ ë‹¨ì–´ ë¶„í¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ì˜ ë²¡í„° í‘œí˜„ì´ ê²°ì •
- ë¶„ì‚°í‘œí˜„ì€ í¬ê²Œ 'ì›í•« ì¸ì½”ë”©', 'ì„ë² ë”©'ì´ ìˆìŒ

#### ì›Ÿ-í•« ì¸ì½”ë”©(One-hot Encoding)
- "I am a student" ë¼ëŠ” ë¬¸ì¥ì„ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ í‘œí˜„
  - I : [1 0 0 0]
  - am : [0 1 0 0]
  - a : [0 0 1 0]
  - student : [0 0 0 1]

- ì¥ì  : ì§ê´€ì ìœ¼ë¡œ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆìŒ
- ë‹¨ì 
  - ë‹¨ì–´ê°€ ë§ì•„ì§€ë©´ ì°¨ì›ì´ ëŠ˜ì–´ë‚¨
  - ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ë¥¼ êµ¬í•  ìˆ˜ ì—†ìŒ
  - [ì½”ì‚¬ì¸ìœ ì‚¬ë„](#ì½”ì‚¬ì¸-ìœ ì‚¬ë„consine-similarity) ì°¸ì¡°

#### ì„ë² ë”©(Embedding)
- ë‹¨ì–´ë¥¼ ê³ ì • ê¸¸ì´ì˜ ë²¡í„°, ì¦‰ ì°¨ì›ì´ ì¼ì •í•œ ë²¡í„°ë¡œ ë‚˜íƒ€ë‚´ëŠ” ê²ƒ
- ì›Ÿ-í•« ì¸ì½”ë”©ì€ 0,1ë¡œ ë‚˜íƒ€ë‚˜ì§€ë§Œ ì„ë² ë”©ì€ [0.04227, -0.0033, 0.1607, -0.0236, ...] ê°™ì€ ì—°ì†ì ì¸ ìˆ˜ í˜•íƒœë¡œ ë‚˜íƒ€ë‚¨

#### Word2Vec
- ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì„ë² ë”© ë°©ë²•
- CBoW ì™€ Skip-gram ì˜ ë‘ ê°€ì§€ ë°©ë²•ì´ ìˆìŒ

![image](https://blog.kakaocdn.net/dn/2Qj2I/btrb7zSiErG/fxZChADnf1iQ7zCx43W5o0/img.png){: .align-center width="70%"} 

- Skip-gramì´ ì„±ëŠ¥ì´ ì¢‹ì•„ì„œ ë” ìì£¼ ì‚¬ìš© ë¨
- Word2Vec ì˜ ë‹¨ì 
  - ë‹¨ì–´ ì§‘í•©ì— ì§€ì •í•˜ì§€ ì•Šì€ ë‹¨ì–´ëŠ” ë²¡í„°í™” í•  ìˆ˜ ì—†ìŒ(OOV)




<br><br><br><br>

## N323 : Language Modeling with RNN
{: style="text-align: center;"}

<br><br><br><br>




### ğŸ† í•™ìŠµ ëª©í‘œ
#### Level 1 : ğŸ“ Lecture Note ì— ìˆëŠ” ì£¼ìš” ê°œë…ê³¼ íë¦„ì„ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•´ ì´í•´í•˜ê³ , í†µê³„ ê¸°ë°˜ ì–¸ì–´ëª¨ë¸ì˜ í•œê³„ì ì„ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ìˆœí™˜ ì‹ ê²½ë§(Recurrent Neural Network, RNN)ì˜ êµ¬ì¡°, ì‘ë™ ë°©ì‹ ë° í•œê³„ì ì— ëŒ€í•´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- LSTMê³¼ GRUê°€ ê³ ì•ˆëœ ë°°ê²½ê³¼ êµ¬ì¡°ë¥¼ ì—°ê´€ì§€ì–´ ì„¤ëª…í•  ìˆ˜ ìˆìœ¼ë©°, ë‘ ëª¨ë¸ì˜ ì°¨ì´ì— ëŒ€í•´ì„œë„ ê°„ëµíˆ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- Attentionì˜ ì¥ì ê³¼ Attention ìœ¼ë¡œë„ í•´ê²°í•  ìˆ˜ ì—†ëŠ” RNNì˜ êµ¬ì¡°ì  ë‹¨ì ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ê°œë…ê³¼ ì½”ë”© Quizë¥¼ í†µí•´ì„œ ìœ„ í•™ìŠµ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ì˜€ëŠ”ì§€ ì ê²€í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Level 2 : ğŸ“ Lecture Note ë‚´ìš©ì— ëŒ€í•´ ì‹¬ë„ìˆê²Œ ì´í•´í•˜ê³  Note ë‚´ì— ìˆëŠ” ì¼ë¶€ ì½”ë“œì— ëŒ€í•´ì„œë„ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- RNNì„ Python ìœ¼ë¡œ êµ¬í˜„í•œ ì½”ë“œë¥¼ ë³´ê³  RNNì˜ êµ¬ì¡°ì™€ ì—®ì–´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- LSTMì˜ ê° Gateì˜ ì“°ì„ìƒˆì™€ ì‘ë™ë°©ì‹ì— ëŒ€í•´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- Quiz - ì‹¬í™” ë° Discussionì„ í†µí•´ì„œ ìœ„ í•™ìŠµ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ì˜€ëŠ”ì§€ ì ê²€í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Level 3 : ğŸ“ Lecture Note ë‚´ìš©ì„ ì½”ë“œ ë ˆë²¨ì—ì„œ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- LSTM ë° Attention ì±•í„°ì—ì„œ Keras Example ì½”ë“œë¥¼ ë³´ê³  ì´í•´í•˜ë©° ì¤‘ìš”í•œ ë¼ì¸ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



### ì˜¤ëŠ˜ì˜ í‚¤ì›Œë“œ
- ì–¸ì–´ ëª¨ë¸, ìˆœí™˜ ì‹ ê²½ë§, LSTM & GRU, Attention



### ê³µë¶€í•œ ë‚´ìš©


#### ì–¸ì–´ ëª¨ë¸ (Language Model)
- ë‹¨ì–´ì˜ í™•ë¥ ì„ ê³„ì‚°í•˜ì—¬ ë¬¸ì¥ì„ ë§Œë“œëŠ” ëª¨ë¸
- ì˜ˆë¥¼ ë“¤ì–´, Word2Vecì˜ CBoWëŠ” ì£¼ë³€ ë‹¨ì–´ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¨ì–´ì˜ í™•ë¥ ì„ ì¶”ì •
  - $P(w_t \vert w_{t-2},w_{t-1},w_{t+1},w_{t+2})$

- í†µê³„ì  ì–¸ì–´ëª¨ë¸(Statistical Language Model, SLM)
  - ë‹¨ì–´ì˜ ë“±ì¥ íšŸìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¡°ê±´ë¶€ í™•ë¥ ì„ ê³„ì‚°
  - ë‹¨ì  : í¬ì†Œ ë¬¸ì œ
  - ë‹¨ì  ê·¹ë³µ ë°©ì•ˆ : N-gram, ìŠ¤ë¬´ë”©(smoothing), ë°±ì˜¤í”„(back-off)

- ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸(Neural Langauge Model)
  - íšŸìˆ˜ ê¸°ë°˜ ëŒ€ì‹  Word2Vecì´ë‚˜ fastText ë“±ì˜ ì¶œë ¥ê°’ì¸ ì„ë² ë”© ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í™•ë¥  ê³„ì‚°


#### ìˆœí™˜ ì‹ ê²½ë§ (Recurrent Neural Network, RNN)
- ì—°ì†í˜• ë°ì´í„° (Sequential Data)
  - ì–´ë–¤ ìˆœì„œë¡œ ì˜¤ëŠëƒì— ë”°ë¼ì„œ ë‹¨ìœ„ì˜ ì˜ë¯¸ê°€ ë‹¬ë¼ì§€ëŠ” ë°ì´í„°
  - ìˆœì„œí˜• ë°ì´í„°ë¡œ ì´í•´
  - ìˆœí™˜ ì‹ ê²½ë§ì€ ì—°ì†í˜•(Sequential) ë°ì´í„°ë¥¼ ì˜ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ê³ ì•ˆëœ ì‹ ê²½ë§

![image](https://www.easy-tensorflow.com/images/NN/01.png){: .align-center width="60%"}  

- RNNì˜ ì¥ì 
  - ëª¨ë¸ì´ ê°„ë‹¨
  - ì´ë¡ ìƒ ì–´ë–¤ ê¸¸ì´ì˜ sequential ë°ì´í„°ë¼ë„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ
- RNNì˜ ë‹¨ì 
  - ë³‘ë ¬í™”(parallelization) ë¶ˆê°€ëŠ¥
  - ê¸°ìš¸ê¸° í­ë°œ(Exploding Gradient), ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)
    - ì—­ì „íŒŒ ê³¼ì •ì—ì„œ RNNì˜ í™œì„±í™” í•¨ìˆ˜ì¸ tanhì˜ ë¯¸ë¶„ê°’ì„ ì „ë‹¬ ë°›ëŠ”ë°, ì´ ë¯¸ë¶„ê°’ì´ ë°˜ë³µí•´ì„œ ì œê³±í•´ì§€ëŠ” ê³¼ì •ì—ì„œ ê°’ì´ í­ë°œí•˜ê±°ë‚˜ ì†Œì‹¤ë˜ê²Œ ë¨
    - ì¥ë‹¨ê¸° ê¸°ì–µë§(Long-Short Term Memory, LSTM)ì˜ Cell Stateë¡œ ê·¹ë³µ


#### LSTM(Long Short Term Memory, ì¥ë‹¨ê¸°ê¸°ì–µë§)

-$C_{t-1}$ (Cell-State) ì™€ $h_{t-1}$ (Hidden-State) ë¡œ êµ¬ì„±
  - Hidden-State : RNNì—ì„œ Sequential ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” ë²¡í„°
  - Cell-State : RNNì˜ ë‹¨ì ì¸ ê¸°ìš¸ê¸°ì†Œì‹¤Â·í­ë°œì˜ ë¬¸ì œë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•˜ì—¬ ì¶”ê°€
    - í™œì„±í™” í•¨ìˆ˜ë¥¼ ê±°ì¹˜ì§€ ì•Šì•„ì„œ ì—­ì „íŒŒê³¼ì •ì—ì„œ ê¸°ìš¸ê¸°ì†Œì‹¤Â·í­ë°œì˜ ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•ŠìŒ

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile5.uf.tistory.com%2Fimage%2F9905CF385BD5F5EC027F20){: .align-center width="60%"} 

1. forget gate ($f_t$) : ê³¼ê±° ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ìœ ì§€í•  ê²ƒì¸ê°€?
2. input gate ($i_t$) : ìƒˆë¡œ ì…ë ¥ëœ ì •ë³´ëŠ” ì–¼ë§ˆë§Œí¼ í™œìš©í•  ê²ƒì¸ê°€?
3. output gate ($o_t$) : ë‘ ì •ë³´ë¥¼ ê³„ì‚°í•˜ì—¬ ë‚˜ì˜¨ ì¶œë ¥ ì •ë³´ë¥¼ ì–¼ë§ˆë§Œí¼ ë„˜ê²¨ì¤„ ê²ƒì¸ê°€?


#### GRU
- LSTMì˜ ê°„ì†Œí•œ ë²„ì „

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile7.uf.tistory.com%2Fimage%2F99F0EC3E5BD5F6460255CF){: .align-center width="60%"} 


1. LSTMì—ì„œ ìˆì—ˆë˜ Cell-Stateê°€ ì‚¬ë¼ì¡ŒìŒ  
- Cell-State ë²¡í„° $c_t$ â€‹ì™€ hidden-state ë²¡í„° $h_t$â€‹ê°€ í•˜ë‚˜ì˜ ë²¡í„° $h_t$â€‹ë¡œ í†µì¼

1. í•˜ë‚˜ì˜ Gate $z_t$ê°€ forget, input gateë¥¼ ëª¨ë‘ ì œì–´
- $z_t$ê°€ 1ì´ë©´ forget ê²Œì´íŠ¸ê°€ ì—´ë¦¬ê³ , input ê²Œì´íŠ¸ê°€ ë‹«íˆê²Œ ë˜ëŠ” ê²ƒê³¼ ê°™ì€ íš¨ê³¼
- ë°˜ëŒ€ë¡œ $z_t$ê°€ 0ì´ë©´ input ê²Œì´íŠ¸ë§Œ ì—´ë¦¬ëŠ” íš¨ê³¼

1. GRU ì…€ì—ì„œëŠ” output ê²Œì´íŠ¸ê°€ ì—†ì–´ì¡ŒìŒ  
- ëŒ€ì‹ ì— ì „ì²´ ìƒíƒœ ë²¡í„° $h_t$ ê°€ ê° time-stepì—ì„œ ì¶œë ¥ë˜ë©°, ì´ì „ ìƒíƒœì˜ $h_{t-1}$ ì˜ ì–´ëŠ ë¶€ë¶„ì´ ì¶œë ¥ë  ì§€ ìƒˆë¡­ê²Œ ì œì–´í•˜ëŠ” Gateì¸ $r_t$ ê°€ ì¶”ê°€


#### Attention
- ê¸°ì¡´ RNN ê¸°ë°˜(LSTM, GRU) ë²ˆì—­ ëª¨ë¸ì˜ ë‹¨ì 
  - ì¥ê¸° ì˜ì¡´ì„±(Long-term dependency) : ë¬¸ì¥ì´ ê¸¸ì–´ì§ˆ ê²½ìš° ì• ë‹¨ì–´ì˜ ì •ë³´ë¥¼ ìƒì–´ë²„ë¦¬ê²Œ ë˜ëŠ” í˜„ìƒ

- ê¸°ì¡´ RNN ê¸°ë°˜ Sequential ëª¨ë¸
  - ìˆœì°¨ì ìœ¼ë¡œ Hidden-Stateë¥¼ ì „ë‹¬í•˜ëŠ” ë°©ì‹
  - ë¬¸ì¥ì´ ê¸¸ì–´ì§€ë©´ Hidden-Stateê°€ ì—…ë°ì´íŠ¸ ë˜ë©´ì„œ ì• ë‹¨ì–´ì˜ ì •ë³´ê°€ ê±°ì˜ ì—†ì–´ì§€ê²Œ ë¨ : ì¥ê¸° ì˜ì¡´ì„±

![image](https://user-images.githubusercontent.com/45377884/86040995-f27b4800-ba7f-11ea-8ca1-67b2517573eb.gif){: .align-center width="80%"}  


- Attentionì˜ ì‘ë™ ì›ë¦¬
  - ê° ì¸ì½”ë”ì˜ Stepë§ˆë‹¤ ìƒì„±ë˜ëŠ” Hidden-Stateë¥¼ ë²¡í„°ë¥¼ ê°„ì§í•˜ê³  ì¼ë¶€ëŠ” ì „ë‹¬
  - Nê°œì˜ ëª¨ë“  ë‹¨ì–´ê°€ ì…ë ¥ë˜ëŠ” Stepì´ ëë‚˜ë©´ ìƒì„±ëœ Nê°œì˜ Hidden-Stateë¥¼ ëª¨ë‘ ë””ì½”ë”ì— ì „ë‹¬
    - ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°

![image](https://user-images.githubusercontent.com/45377884/86040873-b942d800-ba7f-11ea-9f59-ee23923f777e.gif){: .align-center width="80%"}  

- ë””ì½”ë”ì—ì„œ Attentionì´ ë™ì‘í•˜ëŠ” ì›ë¦¬
  1. ë””ì½”ë”ì˜ Hidden-State ë²¡í„°ì™€ ì¸ì½”ë”ì—ì„œ ë„˜ì–´ì˜¨ Hidden-State ë²¡í„°ë¥¼ ë‚´ì 
  2. ë‚´ì í•œ ê°’ì— Softmaxí•¨ìˆ˜ ì·¨í•¨
  3. ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì·¨í•œ ê°’ì— ì¸ì½”ë”ì—ì„œ ë„˜ì–´ì˜¨ Hidden-sateë²¡í„°ë¥¼ ë‚´ì 
  4. 3ì—ì„œ ìƒì„±ëœ ë²¡í„°ì™€ ë””ì½”ë”ì˜ Hidden-Satate ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶œë ¥ ë‹¨ì–´ë¥¼ ê²°ì •










#### [tf.keras.layers.Embedding()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)
- tf.keras.layers.Embedding(
    input_dim,  
    output_dim,  
    embeddings_initializer='uniform',  
    embeddings_regularizer=None,  
    activity_regularizer=None,  
    embeddings_constraint=None,  
    mask_zero=False,  
    input_length=None,  
    **kwargs  
)




#### [tf.keras.layers.LSTM()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)

- tf.keras.layers.LSTM(  
    units,  
    activation='tanh',  
    recurrent_activation='sigmoid',  
    use_bias=True,  
    kernel_initializer='glorot_uniform',  
    recurrent_initializer='orthogonal',  
    bias_initializer='zeros',  
    unit_forget_bias=True,  
    kernel_regularizer=None,  
    recurrent_regularizer=None,  
    bias_regularizer=None,  
    activity_regularizer=None,  
    kernel_constraint=None,  
    recurrent_constraint=None,  
    bias_constraint=None,  
    dropout=0.0,  
    recurrent_dropout=0.0,  
    return_sequences=False,  
    return_state=False,  
    go_backwards=False,  
    stateful=False,  
    time_major=False,  
    unroll=False,  
    **kwargs  
)




#### [spacy ì˜ tokenizer()](https://spacy.io/api/tokenizer)  
- 



#### [keras ì˜ tokenizer()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)  
- 





<br><br><br><br>

## N324 : Transformer
{: style="text-align: center;"}

<br><br><br><br>


### ğŸ† í•™ìŠµ ëª©í‘œ

#### Level 1 : Lecture Note ì— ìˆëŠ” ì£¼ìš” ê°œë…ì„ ì„¤ëª…í•  ìˆ˜ ìˆìœ¼ë©° ì˜ˆì œ ì½”ë“œë¥¼ ì´í•´í•˜ê³  ì¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ê¸°ì¡´ RNNê³¼ ë¹„êµí•˜ì—¬ Transformerê°€ ê°€ì§€ëŠ” ì¥ì ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.
- Positional Encodingì„ ì ìš©í•˜ëŠ” ì´ìœ ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.
- ê°œë…ê³¼ ì½”ë”© Quizë¥¼ í†µí•´ì„œ ìœ„ í•™ìŠµ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ì˜€ëŠ”ì§€ ì ê²€í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Level 2 : ğŸ” Reference ë‚´ìš©ì„ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸(Pre-trained LM)ì˜ Pre-trainingê³¼ Fine-tuningì€ ë¬´ì—‡ì´ê³  ê°ê° ì–´ë–¤ ì¢…ë¥˜ì˜ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ëŠ” ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.
- GPT, BERTëŠ” Transformerë¥¼ ì–´ë–»ê²Œ ë³€í˜•í•œ ê²ƒì¸ì§€ì™€ ë‘ êµ¬ì¡°ì˜ ì°¨ì´ê°€ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.
- BERT ì‚¬ì „ í•™ìŠµ ë°©ë²•ì¸ MLM(Masked Language Model) ê³¼ NSP(Next Sentence Prediction)ì— ëŒ€í•´ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.
- ê°œë…ê³¼ ì½”ë”© Quiz ë° Discussionì„ í†µí•´ì„œ ìœ„ í•™ìŠµ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ì˜€ëŠ”ì§€ ì ê²€í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Level 3 : ğŸ”¥ Reference ë‚´ìš©ì„ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- GPT, BERT ì´í›„ì˜ NLP ëª¨ë¸ ì—°êµ¬ ë°©í–¥ê³¼ í•´ë‹¹í•˜ëŠ” ëª¨ë¸ì˜ ì´ë¦„ì„ ë‚˜ì—´í•  ìˆ˜ ìˆë‹¤.


### ì˜¤ëŠ˜ì˜ í‚¤ì›Œë“œ
- íŠ¸ëœìŠ¤í¬ë¨¸


### ê³µë¶€í•œ ë‚´ìš©
#### Attention is All You Need
- 2017ë…„, Attention ë©”ì»¤ë‹ˆì¦˜ì„ ê·¹ëŒ€í™”í•œ íŠ¸ëœìŠ¤í¬ë¨¸(Transformer) ëª¨ë¸ ë°œí‘œ
- ê¸°ì¡´ RNNì´ ê·¹ë³µí•˜ì§€ ëª»í•œ ìˆœì°¨ì  ëª¨ë¸ì´ë¼ëŠ” ì ì„ ê·¹ë³µ
  - íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ë³‘ë ¬í™” ê°€ëŠ¥ ğŸ‘‰ GPU ì—°ì‚° ê°€ëŠ¥

- ì¸ì½”ë” : Multi-Head (Self) Attention, Feed Forward
- ë””ì½”ë” : Masked Multi-Head (Self) Attention, Multi-Head (Encoder-Decoder) Attention, Feed Forward

![image](https://miro.medium.com/max/1400/1*BHzGVskWGS_3jEcYYi6miQ.png){: .align-center width="70%"} 

1. Positional Encoding 
- ë‹¨ì–´ì˜ ìƒëŒ€ì ì¸ ìœ„ì¹˜ ì •ë³´ë¥¼ ë‹´ì€ ë²¡í„°  
- $\begin{aligned}
\text{PE}_{\text{pos},2i} &= \sin \bigg(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\bigg)\end{aligned}$  
- $\begin{aligned}\text{PE}_{\text{pos},2i+1} &= \cos \bigg(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\bigg)
\end{aligned}$

2. Self-Attention
- **ì¿¼ë¦¬(q)** : ë¶„ì„í•˜ê³ ì í•˜ëŠ” ë‹¨ì–´ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ë²¡í„°
- **í‚¤(k)** : ê° ë‹¨ì–´ê°€ ì¿¼ë¦¬ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ì™€ ì–¼ë§ˆë‚˜ ì—°ê´€ìˆëŠ” ì§€ë¥¼ ë¹„êµí•˜ê¸° ìœ„í•œ ê°€ì¤‘ì¹˜ ë²¡í„°
- **ë°¸ë¥˜(v)** : ê° ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ì‚´ë ¤ì£¼ê¸° ìœ„í•œ ê°€ì¤‘ì¹˜ ë²¡í„°
- QKV ì˜ Attention ìˆœì„œ
  1. ë‹¨ì–´ì˜ ì¿¼ë¦¬ ë²¡í„°ì™€ ëª¨ë“  ë‹¨ì–´ì˜ í‚¤ ë²¡í„°ë¥¼ ë‚´ì  = Attention Score
  2. Attention Score ë¥¼ QKVì˜ ì°¨ì›ì¸ $d_k$ì˜ ì œê³±ê·¼$(\sqrt{d_k})$ìœ¼ë¡œ ë‚˜ëˆ”  
      - $Attention Score\over{ \sqrt{ d_k } }$
      - Smoothing íš¨ê³¼
  3. 2ì˜ ê²°ê³¼ì— Softmax()
      - $SoftMax({ { Attention Score }\over{\sqrt{d_k } } })$
  4. 3ì— ê° ë°¸ë¥˜ ë²¡í„°ì™€ ë‚´ì 



3. Multi-Head Attention
- ì—¬ëŸ¬ ê°œì˜ Attention ë©”ì»¤ë‹ˆì¦˜ì„ ë™ì‹œì— ë³‘ë ¬ì ìœ¼ë¡œ ì‹¤í–‰
- ì•™ìƒë¸”ê³¼ ìœ ì‚¬í•œ íš¨ê³¼

4. Layer Normalization & Skip Connection  
- Layer normalization
  - Batch normalizationê³¼ ìœ ì‚¬í•œ íš¨ê³¼
- Skip Connection  
  - ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ì •ë³´ê°€ ì†Œì‹¤ë˜ì§€ ì•Šë„ë¡

5. Feed forward neural network
- ì€ë‹‰ì¸µì˜ ì°¨ì›ì´ ëŠ˜ì–´ë‚¬ë‹¤ê°€ ë‹¤ì‹œ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ì¤„ì–´ë“œëŠ” ë‹¨ìˆœí•œ 2ì¸µ ì‹ ê²½ë§

6. Masked Self-Attention
- ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥ë˜ëŠ” RNNê³¼ ë‹¬ë¦¬ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œëŠ” íƒ€ê¹ƒ ë¬¸ì¥ ì—­ì‹œ í•œ ë²ˆì— ì…ë ¥ë˜ê¸° ë•Œë¬¸ì—
í•´ë‹¹ ìœ„ì¹˜ íƒ€ê¹ƒ ë‹¨ì–´ ë’¤ì— ìœ„ì¹˜í•œ ë‹¨ì–´ëŠ” Self-Attentionì— ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ ë§ˆìŠ¤í‚¹(Masking)ì´ í•„ìš”í•¨

![image](http://jalammar.github.io/images/gpt2/transformer-attention-mask.png){: .align-center width="70%"}  

![image](http://jalammar.github.io/images/gpt2/transformer-attention-masked-scores-softmax.png){: .align-center width="70%"}  

7. Encoder-Decoder Attention
- ë””ì½”ë” ë¸”ë¡ì˜ Masked Self-Attentionìœ¼ë¡œë¶€í„° ì¶œë ¥ëœ ë²¡í„°ë¥¼ ì¿¼ë¦¬(Q) ë²¡í„°ë¡œ ì‚¬ìš©
- í‚¤(K)ì™€ ë°¸ë¥˜(V) ë²¡í„°ëŠ” ìµœìƒìœ„(=6ë²ˆì§¸) ì¸ì½”ë” ë¸”ë¡ì—ì„œ ì‚¬ìš©í–ˆë˜ ê°’ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©

8. Linear & Softmax Layer












<br><br><br><br>


## Reference
- [ì§€í”„ì˜ ë²•ì¹™(Zipf's Law)](https://ko.wikipedia.org/wiki/%EC%A7%80%ED%94%84%EC%9D%98_%EB%B2%95%EC%B9%99)  
- [[keras] pad_sequence ì‰½ê²Œ ì„¤ëª…](https://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221242674418&parentCategoryNo=&categoryNo=58&viewDate=&isShowPopularPosts=false&from=postView)
- [Spacy 101](https://course.spacy.io)
- [regex101.com(ì •ê·œì‹ ì—°ìŠµ)](https://regex101.com/)
- [Python RegEx](https://www.w3schools.com/python/python_regex.asp#sub)
- [ì •ê·œ í‘œí˜„ì‹ ì‹œì‘í•˜ê¸°](https://wikidocs.net/4308#_2)


<br><br><br><br>

<center>
<h1>ëê¹Œì§€ ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤ğŸ˜‰</h1>
</center>





