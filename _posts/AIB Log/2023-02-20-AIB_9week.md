---

title: "AI 부트캠프 9주 : Neural Networks"

excerpt: "코드스테이츠와 함께하는 'AI 부트캠프' 9주차 회고"

categories:
  - AIB Log

tags:
  - 개발일지
  - 코딩
  - AI 부트캠프
  - 코드스테이츠

header:
  teaser: /assets/images/aib/codestates-ci.png

last_modified_at: 2023-02-20

---



<br>
<br>
<br>
<br>


![image](https://leeyeonjun85.github.io/home/assets/images/aib/artificial-intelligence-3685928_1920.png){: .align-center width="70%"}  


<br>
<br>
<br>
<br>




# 코드스테이츠와 함께하는 'AI 부트캠프' 9주차




<br><br><br><br>

## 주간회고
{: style="text-align: center;"}

<br><br><br><br>




### 더 공부가 필요한 부분
> 라이브러리가 아닌 사용자함수구현을 통한 딥러닝 연습

### 5F 회고
- 사실(Fact)  
인공신경망에 대하여 배우고 간단한 인공신경망 구성으로 딥러닝 공부를 시작하였다.

- 느낌(Feeling)  
인공신경망의 이론적 원리도 쉽게 이해되지 않고, keras 라이브러리를 이용하여 인공신경망을 구성하는 것도 어려워서 답답함을 느꼈다. 오히려 sklearn의 GridSearchCV()를 이용하여 하이퍼파라미터를 찾을 때 반가움을 느꼈다.

- 교훈(Finding)  
아직은 keras 라이브러리에 익숙치 않기 때문에 어려운 것 같다. 먼저 라이브러리에 익숙해지고, 친해져야 할 것 같다.

- 향후 행동(Future action)  
keras 공식문서의 튜토리얼, 가이드를 통하여 라이브러리에 익숙해지고, 다양한 데이터에 적용하여 모델을 만들어봐야겠다.

- 피드백(Feedback)  
내용



<br><br><br><br>

## N311 : Artificial Neural Network
{: style="text-align: center;"}

<br><br><br><br>




### 🏆 학습 목표
#### Level 1 : Lecture Note 에 있는 주요 개념을 설명할 수 있으며 예제 코드를 이해하고 재현할 수 있습니다.
- 퍼셉트론(Perceptron)의 개념과 구조에 대해 설명할 수 있습니다.
- 신경망을 왜 다층으로 구성해야 하는 지와 신경망 각 층(입력층, 은닉층, 출력층)의 역할에 대해 설명할 수 있습니다.
- MNIST 예제 코드를 이해하고 재현할 수 있습니다.
- 개념과 코딩 Quiz를 통해서 위 학습 목표를 달성하였는지 점검해 볼 수 있습니다.

#### Level 2 : 🔝 Reference 내용을 이해하고, 신경망 모델을 다른 데이터에도 적용할 수 있습니다.
- 가중치 행렬의 Shape 과 신경망 구조에 대해서 이해하고 설명할 수 있습니다.
- 활성화 함수의 공통점과 신경망의 특징인 표현 학습에 대해 이해합니다.
- Discussion 에 있는 코딩 과제를 통해서 위 학습 목표를 달성하였는지 점검해 볼 수 있습니다.

#### Level 3 : 🔥 Reference 내용을 설명할 수 있고 기본적인 신경망 기본 형태를 파이썬 기본 코드로 구축할 수 있습니다.
- 시그모이드 함수의 단점인 기울기 소실 문제와 ReLU 함수를 쓰는 이유에 대해 이해할 수 있습니다.
- 파이썬 기본 코드로 퍼셉트론을 구현할 수 있습니다.



### 키워드
- 퍼셉트론
- 활성화 함수
- 인공신경망
- Tensorflow



### 공부한 내용

#### 퍼셉트론
- 퍼셉트론(Perceoptron) : 인공 신경망을 이루는 가장 기본 단위
- '가중합 계산과정', '활성화 함수 과정' 두 단계로 나뉨
- 가중합
  - 다수의 입력 신호에 대하여 가중치와 편향의 연산
  - 가중치와 편향의 행렬곱(내적)
- 활성화 함수
  - 하나의 신호로 출력
  - 계단 함수(Step function)
    - 임계값(0)을 넘기면 1, 넘지 않으면 0을 출력
    - 임계값에서는 미분 불가능하고, 그 외의 지점에서는 미분값이 0
    - 미분 불가능하여 딥러닝 학습이 어려움

![image](https://i.imgur.com/Pbf8B4B.png){: .align-center width="60%"} 

  - 시그모이드 함수(Sigmoid function)
    - 계단 함수가 미분 불가능한 점을 보완하기 위한 함수
    - 기울기 소실(Vanishing Gradient) 문제

![image](https://i.imgur.com/W884y3K.png){: .align-center width="60%"} 

  - 렐루 함수(ReLU function)
    - 시그모이드의 기울기 소실 문제를 극복하기 위한 함수

![image](https://i.imgur.com/HuorFwZ.png){: .align-center width="60%"}  

  - 소프트맥스 함수(Softmax function)
    - 다중 분류(Multi-classification) 문제
    - 모든 클래스의 값의 합이 1이 되는 확률값 반환


#### 논리게이트로 퍼셉트론 알아보기  

- AND GATE : 입력 신호가 모두 1(True)일 때 1(True)을 출력

![image](https://i.imgur.com/uMFnwaf.png){: .align-center width="60%"}  

- NAND GATE : 입력 신호가 모두 0(False)일 때 1(True)을 출력

![image](https://i.imgur.com/6MiCoFk.png){: .align-center width="60%"}  

- OR GATE : 입력 신호 중 하나만 1(True)이어도 1(True)을 출력

![image](https://i.imgur.com/xG3OLEI.png){: .align-center width="60%"}  

- XOR GATE : 입력 신호가 다를 경우 1(True)을 출력

![image](https://i.imgur.com/CsnYMeV.png){: .align-center width="60%"}  

- 신경망이 논의되던 초기에는 퍼셉트론의 한계로 XOR GATE의 표현이 불가능한 점이 지적되었음
- 하지만 AND, NAND, OR GATE 를 다층으로 활용하면 XOR GATE를 구할 수 있음

![image](https://i.imgur.com/lydr6WY.png){: .align-center width="60%"}  

- NAND(A, B)

|A|B|NAND OUT|
|---|---|---|
| 0 | 0 | 1 |
| 0 | 1 | 1 |
| 1 | 0 | 1 |
| 1 | 1 | 0 |

- OR(A, B)

|A|B|OR OUT|
|---|---|---|
| 0 | 0 | 0 |
| 0 | 1 | 1 |
| 1 | 0 | 1 |
| 1 | 1 | 1 |

- AND(NAND(A, B), OR(A, B))

|NAND OUT|OR OUT|AND OUT|
|---|---|---|
| 1 | 0 | 0 |
| 1 | 1 | 1 |
| 1 | 1 | 1 |
| 0 | 1 | 0 |

- XOR(A, B)

|A|B|XOR OUT|
|---|---|---|
| 0 | 0 | 0 |
| 0 | 1 | 1 |
| 1 | 0 | 1 |
| 1 | 1 | 0 |

- 결국, AND(NAND(A, B), OR(A, B)) = XOR(A, B)

### 인공신경망

- 다층 퍼셉트론 신경망(Multi-Layer Perceptron, MLP)
  - 퍼셉트론을 여러 개의 층으로 쌓아 구축한 신경망

- 인공신경망(Artificial Neural Networks)
  - 실제 신경계를 모사하여 만들어진 계산 모델

![image](https://i.imgur.com/ADcl9EN.png){: .align-center width="60%"}  

- 입력층(Input Layer)
  - 특성(Feature)에 따라 입력층 노드의 수가 결정

- 은닉층(Hidden Layers)
  - 가중합이 계산 되는 층
  - 자유롭게 노드 수를 구성

- 출력층(Output Layer)
  - 문제 종류에 따라서 출력층을 잘 설계해야 함
  - 분류(이진 / 다중) / 회귀




<br><br><br><br>

## N312 : Training Neural Network
{: style="text-align: center;"}

<br><br><br><br>


### 🏆 학습 목표
#### Level 1 : Lecture Note 에 있는 주요 개념을 설명할 수 있으며 예제 코드를 이해하고 재현할 수 있습니다.
- 신경망이 학습되는 메커니즘(순전파, 손실 계산, 역전파)에 대해 적절한 비유를 들어 설명할 수 있습니다.
- 경사 하강법(Gradient Descent, GD)을 통해 가중치가 갱신되는 과정을 대략적으로 설명할 수 있습니다.
- 옵티마이저(Optimizer)의 개념과 확률적 경사 하강법(Stochastic Gradient Descent, SGD) 및 미니 배치 경사 하강법의 개념에 대해 설명할 수 있습니다.
- 개념과 코딩 Quiz를 통해서 위 학습 목표를 달성하였는지 점검해 볼 수 있습니다.

#### Level 2 : 🔝 Reference 내용을 이해하고, 신경망 모델을 다른 데이터에도 적용할 수 있습니다.
- 편미분(Partial Derivatives)과 연쇄 법칙(Chain Rule)에 대해 이해하고 곱셈 노드, 덧셈 노드 및 활성화 함수에 대한 미분 예제를 풀 수 있습니다.
- 편미분과 연쇄 법칙을 사용하여 역전파 과정을 설명할 수 있습니다.
- Discussion 학습 자료 작성 과제를 통해서 위 학습 목표를 달성하였는지 점검해 볼 수 있습니다.

#### Level 3 : 🔥 Reference 내용을 이해하고, 신경망 학습 과정을 파이썬 기본 코드로 구현할 수 있습니다.
- 🔝 Reference 에서 이론적으로 이해한 내용을 파이썬 코드로 구현할 수 있습니다.
- 구현한 함수를 모두 엮어 신경망 학습을 파이썬 코드로 작성할 수 있습니다.



### 키워드
- 순전파
- 역전파
- 손실함수
- 경사하강법
- 옵티마이저



### 공부한 내용

- 신경망의 학습과정
  - 순전파 ➡ 손실계산 ➡ 역전파 = 1 Iteration : 가중치 갱신

![image](https://i.imgur.com/dlGareT.gif){: .align-center width="70%"}  

#### 순전파(Forward Propagation)
- 입력된 데이터가 각 층에서 연산을 거쳐 출력하는 과정
  1. 데이터 입력
  2. 가중치-편향 연산
  3. 활성화 함수를 통하여 다음 층에 전달

#### 손실 함수(Loss Function)
- 순전파로 출력된 값과 실제 타겟값의 차이에 대한 함수
- 회귀 : MSE, MAE
- 이진분류 : binary_crossentropy
- 다중분류 : categorical_crossentropy, sparse_categorical_crossentropy

#### 역전파(Backward Propagation)
- 순전파와 반대 방향으로 손실 정보를 전달하며 가중치를 얼마나 업데이트 해야할지를 구하는 과정
- 경사하강법을 이용하여 가중치 수정값 결정


#### 경사 하강법(Gradient Descent)

- Iteration 마다 손실함수의 도함수를 계산하여 가중치를 변경

![image](https://i.imgur.com/ic91umJ.png){: .align-center width="60%"}  

- 예시
  - 손실함수 = MSE
  - 활성함수 = Sigmoid
  - 입력노드, 출력노드 = 2개

- 손실함수

- $E_{total} = \frac{1}{2}(t_1 - Y_1)^2 + \frac{1}{2}(t_2 - Y_1)^2$

- $Y_1 = Sigmoid(y_1)$  
  - $Y_1' = Sigmoid(y_1)(1 - Sigmoid(y_1))$  
  - [시그모이드 함수의 미분](http://taewan.kim/post/sigmoid_diff/)
- $y_1 = H_1V_1 + H_2V_3$  
  - $y_1' = H_1$  

- $S(x) = \frac{1}{1 + e^{-x}}$







<br><br><br><br>

## N313 : 더 나은 신경망 학습을 위한 방법들
{: style="text-align: center;"}

<br><br><br><br>

### 🏆 학습 목표  
#### Level 1 : Lecture Note 에 있는 주요 개념을 설명할 수 있으며 예제 코드를 이해하고 재현할 수 있습니다.  
- 학습률(Learning rate)의 개념과 학습률이 너무 크거나 작은 경우 발생하는 문제에 대해 설명할 수 있습니다.
- 활성화 함수에 맞는 가중치 초기화(Weight Initialization)을 매칭할 수 있습니다.
- 신경망에 적용할 수 있는 과적합(Overfitting)을 방지할 수 있는 방법(Weight Decay, Dropout, Early stopping)의 개념에 대해 설명할 수 있고 이를 Keras 로 적용할 수 있습니다.
- 개념과 코딩 Quiz를 통해서 위 학습 목표를 달성하였는지 점검해 볼 수 있습니다.

#### Level 2 : 🔝 Reference 내용을 이해하고, 신경망 모델을 다른 데이터에도 적용할 수 있습니다.  
- 지난 강의에서 배운 내용 외에 해당하는 Optimizer 의 특징에 대해 개략적으로 설명할 수 있습니다.
- Dropout 의 효과와 Evaluation 단계에서 Dropout 이 어떻게 적용되는 지 설명할 수 있습니다.
- Discussion을 통해서 위 학습 목표를 달성하였는지 점검해 볼 수 있습니다.bundle exec jekyll serve

#### Level 3 : 🔥 Reference 내용을 이해하고, 신경망 학습 과정을 파이썬 기본 코드로 구현할 수 있습니다.  
- 배치 정규화(Batch Normalization)를 이해하고 이를 Keras 코드로 신경망에 적용할 수 있습니다.
- Discussion의 코딩 과제에 배치 정규화를 적용해봄으로써 위 학습 목표를 달성하였는지 점검해 볼 수 있습니다.





### 키워드
- 학습률
- 가중치
- 드롭아웃
- 조기 종료



### 공부한 내용

#### 학습률
- 역전파 과정 중 손실 계산  
- $\theta_{ i,j+1 } = \theta_{ i,j } - \eta { { \delta }\over{ \delta \theta_{ i,j } } }J(\theta)$
  - $\theta_{ i,j+1 }$ : 새롭게 생신된 가중치
  - $\theta_{ i,j }$ : 갱신 전 가중치
  - $\eta$ : 학습률
  - ${ { \delta }\over{ \delta \theta_{ i,j } } }J(\theta)$ : 해당 지점에서의 기울기  
- 학습률이 너무 작으면
  - 학습시간이 오래걸림
  - 최저점에 이르기 전 학습이 끝날 수 있음
- 학습률이 너무 크면
  - 최저점에 이르지 않고 발산 할 수 있음

#### 학습률 감소 계획
- warmup 이후 학습률을 감소시켜가며 신경망 학습 진행
- Step Decay, Cosine Decay가 있음

![image](https://i.imgur.com/Ida3BHl.png){: .align-center width="60%"}  

#### 가중치 초기화  
- 가중치 초기화(Weight initialization)는 활성화 값의 분포와 관련
- Xavier 초기화(glorot) : 표준편차가 $1\over{\sqrt{n}}$인 정규분포로 초기화
  - Sigmoid 에 주로 사용
- He 초기화 : 표준편차가 $\frac{2}{\sqrt{n}}$인 정규분포로 초기화
  - ReLU 에 주로 사용

#### 과적합 방지
1. 가중치 감소(Weight Decay)
- L1(LASSO), L2(Ridge) Regularization
- kernel_regularizer, activity_regularizer 가 있음  

2. Dropout
- 레이어 노드 중 일부를 사용하지 않으면서 학습을 진행

![image](https://i.imgur.com/rAyIZxV.png){: .align-center width="70%"}  

3. 조기 종료(Early Stopping)
- 과적합 발생시 학습 종료




<br><br><br><br>

## N314 : Hyperparameter Tuning
{: style="text-align: center;"}

<br><br><br><br>




### 🏆 학습 목표
#### Level 1 : Lecture Note 에 있는 주요 개념을 설명할 수 있으며 예제 코드를 이해하고 재현할 수 있습니다.
- 신경망에 교차 검증(Cross-Validation)을 적용할 수 있습니다.
- 하이퍼파라미터 탐색법 중 Grid 탐색법과 Random 탐색법에 대해 말하고 둘을 비교하여 설명할 수 있습니다.
- Scikit-learn 와 Keras Tuner 를 사용하여 신경망의 하이퍼파라미터를 탐색할 수 있습니다.
- 코딩 Quiz를 통해서 위 학습 목표를 달성하였는지 점검해 볼 수 있습니다.

#### Level 2 : 🔝 Reference 내용을 이해하며, 신경망의 주요 용어들에 대해 설명할 수 있습니다.
- 신경망 주요 용어에 대해 한 줄 이상으로 설명해봅시다.
- Discussion 내용을 통하여 위 학습 목표를 달성하였는지 점검해 볼 수 있습니다.

#### Level 3 : 🔥 Reference 내용을 이해하여 실험 계획 라이브러리를 적용할 수 있습니다.
- 실험 계획 라이브러리인 WandB 의 사용법을 익히고 Keras 를 엮어서 사용해 볼 수 있습니다.
- WandB 를 사용하여 하이퍼파라미터 탐색을 적용해보면서 해당 학습 목표를 달성하였는지 점검해 볼 수 있습니다.


### 키워드
- 교차 검증
- 하이퍼파라미터
- GridSearch
- Keras Tuner

### 공부한 내용
#### 교차검증
- sklearn의 KFold(), StatifiedKFold()
- StatifiedKFold()은 타겟의 비율을 보존하면서 데이터를 분할

#### 하이퍼파라미터
- 인공신경망에서 주로 다루어지는 하이퍼파라미터
1. 은닉층(Hidden layer)의 수
   - 일반적으로 딥러닝은 은닉층이 2개 이상
2. 은닉층의 노드(Node) 수
   - 인공신경망의 퍼셉트론으로 가중치와 편향의 연산이 이루어지는 곳
3. epoch
   - 순전파의 연산, 역전파의 업데이트 과정이 이루어지는 횟수
4. 학습률(learning rate)
   - 역전파에서 가중치를 업데이트 할 때 학습비율
5. 배치 크기(batch_size)
   - Data Size = batch_size * iteration
   - Total iteration = iteration * epoch

- 이 밖에도...
  - 옵티마이저(optimizers)
  - 활성화 함수(activation)
  - Regularization(weight decay, Dropout 등)


#### 파이퍼파라미터 튜닝
- GridSearch, Random Search, Keras Tuner

#### [옵티마이저(optimizers)](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)

![image](https://i.imgur.com/UQfpjpP.png){: .align-center width="70%"}  




<br><br><br><br>  
<center>  
<h1>끝까지 읽어주셔서 감사합니다😉</h1>  
</center>  
<br><br><br><br>  


