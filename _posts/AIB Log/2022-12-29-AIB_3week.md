---
title: "AI ë¶€íŠ¸ìº í”„ 3ì£¼ : ì„ í˜•ëŒ€ìˆ˜"
excerpt: "ì½”ë“œìŠ¤í…Œì´ì¸ ì™€ í•¨ê»˜í•˜ëŠ” 'AI ë¶€íŠ¸ìº í”„' 3ì£¼ì°¨ íšŒê³ "

categories:
  - AIB Log

tags:
  - ê°œë°œì¼ì§€
  - ì½”ë”©
  - AI ë¶€íŠ¸ìº í”„
  - ì½”ë“œìŠ¤í…Œì´ì¸ 

use_math: true

header:
  teaser: /assets/images/aib/codestates-ci.png

last_modified_at: 2022-12-29
---

![image](../../assets/images/etc/graph-3068300_1920.jpg){: .align-center width="50%"} 


# [ì½”ë“œìŠ¤í…Œì´ì¸ ](https://www.codestates.com/)ì™€ í•¨ê»˜í•˜ëŠ” 'AI ë¶€í”„ìº í”„' 3ì£¼ì°¨


<br><br>


## ì „ë°˜ì  íšŒê³ ğŸˆ
> ì½”ë“œìŠ¤í…Œì´ì¸  AIë¶€íŠ¸ìº í”„ AIsection1 sprint3 ê³¼ì •ì— ì°¸ì—¬í•˜ì—¬ ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ ê¸°ì´ˆì ì¸ ê³µë¶€(ì„ í˜•ëŒ€ìˆ˜, ì°¨ì›ì¶•ì†Œ, ê²½ì‚¬í•˜ê°•ë²•)ì™€ ì‹¤ìŠµ(PCA, í´ëŸ¬ìŠ¤í„°ë§)ì„ ì§„í–‰í•˜ì˜€ë‹¤.  
ê³ ë“±í•™êµìˆ˜í•™ê³¼ì •ì´ ë‚˜ì™€ì„œ ë‹¹í™©í–ˆë‹¤. ì†ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ê²ƒì€ ì–´ëŠì •ë„ í•  ìˆ˜ ìˆê² ëŠ”ë°, ì½”ë“œë¡œ êµ¬í˜„í•˜ë ¤ë‹ˆ ë§ì´ ì–´ìƒ‰í–ˆë‹¤. í•˜ì§€ë§Œ ê¾¸ì¤€í•˜ê²Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ê³„ì‚°í•˜ë‹ˆ ë§ì´ ìµìˆ™í•´ì ¸ì„œ í¸í•œëŠë‚Œ, <mark>ë‚´ ê²ƒì´ ëœ ëŠë‚Œ</mark>ì´ ë“¤ì—ˆë‹¤.  
ê³ ë“±í•™êµ ìˆ˜í•™ê³¼ì •ì—ì„œ ë°°ì› ë˜ ë‚´ìš©ë“¤ì´ ë§ì´ ë‚˜ì™”ë‹¤. ê·¸ë¦¬ê³  ì§ì ‘ ìˆ˜í•™ê³µì‹ì„ í™œìš©í•˜ë‹ˆ ì ì  ìµìˆ™í•´ì§€ê³ , ìˆ˜í•™ê³µì‹ì´ ë‚´ê²ƒì´ ëœ ëŠë‚Œì´ì—ˆë‹¤. ì•„ëŠ” ê²ƒì—ì„œ ë§Œì¡±í•˜ì§€ ì•Šê³  ìµìˆ™í•´ì ¸ì„œ ë‚´ê²ƒìœ¼ë¡œ ë§Œë“¤ì–´ì•¼ê² ë‹¤.  
ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì´ í¸ë¦¬í•œ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤. ê·¸ í¸ë¦¬í•œ ê¸°ëŠ¥ì— ìµìˆ™í•´ì ¸ì•¼ê² ë‹¤. ì–´ë–¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì´ ìˆëŠ”ì§€ ì‚´í”¼ê³ , ê¾¸ì¤€í•˜ê²Œ ì‹¤ìŠµí•˜ë©´ì„œ ìµìˆ™í•´ì§€ë©´, ë¹„ë¡œì†Œ ê·¸ ê¸°ëŠ¥ë“¤ì´ ë‚´ê²ƒì´ ë  ê²ƒì´ë‹¤.


<br><br>


## í•™ìŠµë‚´ìš©ğŸ“
### í•™ìŠµ ê¸°ê°„
- Sprint 2ê¸°ê°„ : 2022.12.23.(ê¸ˆ) ~ 29.(ëª©)

### Linear Algebra
- pythonì˜ listì™€ numpyì˜ arrayì˜ ì°¨ì´
  - list : ìˆ˜ì¹˜ì  ì—°ì‚°ì´ ë¶ˆê°€ëŠ¥
  - array : ìˆ˜ì¹˜ì  ì—°ì‚°ì´ ê°€ëŠ¥

- Scalar : ìˆ«ì
- Vector : ìˆœì„œë¥¼ ê°–ëŠ” 1ì°¨ì› í˜•íƒœì˜ ë°°ì—´
  - ë²¡í„°ì˜ í¬ê¸° : ë²¡í„°ì˜ ê¸¸ì´, Norm, length, Magnitude
  - ë²¡í„°ì˜ ë‚´ì  : Dot Product, ê³±í•˜ê³ ë”í•˜ê³ 
  - ë²¡í„°ì˜ ì§êµ : ë‚´ì ì´  0
  - ë‹¨ìœ„ ë²¡í„° : Unit Vector, ê¸¸ì´ê°€ 1ì¸ ë²¡í„°
- Matrix : í–‰ê³¼ ì—´
  - í–‰ë ¬ì˜ ì „ì¹˜ : Transpose
    - $A^T$
    - $(A^T)^T=A$ 
    - [`np.transpose()`](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html)
  - í–‰ë ¬ê³±(Matrix Multiplication)
    - [`np.matmul()`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html)
  - ì •ì‚¬ê° í–‰ë ¬(Square Matrix)
    - í–‰ê³¼ ì—´ì´ã…¡ ìˆ˜ê°€ ë™ì¼í•œ ë©”íŠ¸ë¦­ìŠ¤
  - ëŒ€ê° í–‰ë ¬(Diagonal Matrix)
    - ì£¼ ëŒ€ê°ì„ (principal diagonal)ì„ ì œì™¸í•œ ëª¨ë“  ì„±ë¶„ì´  0 ì¸ ì •ì‚¬ê° í–‰ë ¬
    > $D =
\begin{bmatrix}
a_{1,1} & 0 & 0 \\
0 & a_{2,2} & 0 \\
0 & 0 & a_{3,3} 
\end{bmatrix}$
  - ë‹¨ìœ„ í–‰ë ¬(Identity Matrix)
    - ëŒ€ê° í–‰ë ¬ ì¤‘ì—ì„œ ì£¼ ëŒ€ê°ì„  ì„±ë¶„ì´ ëª¨ë‘  1 ì¸ ë§¤íŠ¸ë¦­ìŠ¤
    - [`np.identity()`](https://numpy.org/doc/stable/reference/generated/numpy.identity.html) ë˜ëŠ” [`np.eye()`](https://numpy.org/devdocs/reference/generated/numpy.eye.html)
  - ì—­í–‰ë ¬(Inverse)
    - ì„ì˜ì˜ ì •ì‚¬ê° í–‰ë ¬ì— ëŒ€í•˜ì—¬ ê³±í–ˆì„ ë•Œ ë‹¨ìœ„ í–‰ë ¬ì´ ë˜ë„ë¡ í•˜ëŠ” í–‰ë ¬
    - $A^{-1}$
    - [`np.linalg.inv()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html)
  - í–‰ë ¬ì‹(Determinant)
    - ì •ì‚¬ê° í–‰ë ¬ $A$ì— ëŒ€í•´ì„œ det($A$) ë˜ëŠ” $|A|$ë¡œ í‘œê¸°
    - [`np.linalg.det()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.det.html)
    - í–‰ë ¬ì‹ì´  0 ì´ë©´ ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ
- Span
  - ì£¼ì–´ì§„ ë‘ ë²¡í„°ì˜ ì¡°í•©ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ëª¨ë“  ê°€ëŠ¥í•œ ë²¡í„°ì˜ ì§‘í•©
  - ë‘ ë²¡í„°ê°€ ì„ í˜• ë…ë¦½ì´ë©´ 2ì°¨ì› span
  - ì„¸ ë²¡í„°ê°€ ì„ í˜• ë…ë¦½ì´ë©´ 3ì°¨ì› span
  - ì„ í˜•ë…ë¦½ì¸ ë‘ ë²¡í„°(ë¹¨ê°•, ì´ˆë¡)ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ë²¡í„°(íŒŒë‘)ë“¤ì˜ ê³µê°„(ì—°ë‘)   
![image](../../assets/images/aib/section1/span_01.png){: .align-center width="70%"}  

- Rank  
  - ê³µê°„(span)ì˜ ì°¨ì›  
  - ì•„ë˜ëŠ” 3ê°œì˜ í–‰ë²¡í„°ë¡œ ì´ë£¨ì–´ì ¸ ìˆì§€ë§Œ í•˜ë‚˜ì˜ ë²¡í„°(ë³´ë¼)ê°€ ë‹¤ë¥¸ ë‘ ë²¡í„°(ì´ˆë¡, íŒŒë‘)ì˜ spanì— ì¢…ì†ë˜ì–´ ìˆì–´ì„œ ê²°ê³¼ì ìœ¼ë¡œ rankëŠ” 2, ê³§ 3ê°œì˜ ë²¡í„°ê°€ 2ì°¨ì› ê³µê°„ì„ ì´ë£¨ëŠ” ëª¨ìŠµ
  - ë¹¨ê°•, ì´ˆë¡, íŒŒë‘ì˜ í‰ë©´ ì‚¼ê°í˜•ì„ ì´ë£¸  
![image](../../assets/images/aib/section1/rank_01.png){: .align-center width="70%"}  
  
<br>

### PCA

#### ê³µë¶„ì‚°ê³¼ ìƒê´€ê³„ìˆ˜
- ë¶„ì‚°(Variance)
    - ë°ì´í„°ê°€ í©ì–´ì ¸ ìˆëŠ” ì •ë„ë¥¼ í•˜ë‚˜ì˜ ê°’ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒ  
    - ë°ì´í„°ê°€ ì„œë¡œ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆì„ìˆ˜ë¡ ë¶„ì‚°ì˜ ê°’ì´ ì»¤ì§  
    - í¸ì°¨ ì œê³±ì˜ í‰ê·   
  - > $\sigma^2 = \frac{\sum{(X_{i} - \overline{X})^{2} } }{N} $ where $\ $ $X_i$:ê´€ì¸¡ê°’ , $\bar{X}$:í‰ê·  , $N$: ê´€ì¸¡ê°’ ê°œìˆ˜  

  - [df.var()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.var.html) ë˜ëŠ” [`np.var()`](https://numpy.org/doc/stable/reference/generated/numpy.var.html)  
  - í¸ì°¨ = (ê´€ì¸¡ê°’) - (í‰ê· )  
  - í¸ì°¨ì˜ í•©ì€ í•­ìƒ  0 ì´ê¸° ë•Œë¬¸ì— í¸ì°¨ì˜ í‰ê· ë„ í•­ìƒ  0  

- ê³µë¶„ì‚°(Covariance)  
  - ê³µë¶„ì‚°ì˜ ê°’ì´ í¬ë‹¤ë©´ ë‘ ë³€ìˆ˜ ê°„ì˜ ì—°ê´€ì„±ì´ í¬ë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆë‹¤!  
  - í•˜ì§€ë§Œ ë¶„ì‚°ì€ ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ì— ì˜í–¥ì„ ë§ì´ ë°›ê¸° ë•Œë¬¸ì— ê°’ì´ í¬ë‹¤ê³ í•´ì„œ ë¬´ì¡°ê±´ ì—°ê´€ì„±ì´ í¬ë‹¤ê³  í•  ìˆ˜ ì—†ë‹¤.  
  - <mark>ì—°ê´€ì„±ì´ ì ë”ë¼ë„ í° ìŠ¤ì¼€ì¼ì„ ê°€ì§€ê³  ìˆë‹¤ë©´ ì—°ê´€ì„±ì´ ë†’ì§€ë§Œ ì‘ì€ ìŠ¤ì¼€ì¼ì„ ê°€ì§„ ë³€ìˆ˜ë“¤ì— ë¹„í•´ì„œ ë†’ì€ ê³µë¶„ì‚°</mark> ê°’ì„ ê°€ì§€ê²Œ ëœë‹¤.

- ë¶„ì‚°-ê³µë¶„ì‚° í–‰ë ¬(variance-covariance matrix)
  - ëª¨ë“  ë³€ìˆ˜ì— ëŒ€í•˜ì—¬ ë¶„ì‚°ê³¼ ê³µë¶„ì‚° ê°’ì„ ë‚˜íƒ€ë‚´ëŠ” ì •ì‚¬ê° í–‰ë ¬
    - ì£¼ ëŒ€ê°ì„  ì„±ë¶„ì€ ìê¸° ìì‹ ì˜ ë¶„ì‚° ê°’
    - ì£¼ ëŒ€ê°ì„  ì´ì™¸ì˜ ì„±ë¶„ì€ ê°€ëŠ¥í•œ ë‘ ë³€ìˆ˜ì˜ ê³µë¶„ì‚° ê°’
  - [`df.cov()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.cov.html) ë˜ëŠ” [`np.cov()`](https://numpy.org/doc/stable/reference/generated/numpy.cov.html)  

- ìƒê´€ê³„ìˆ˜(Correlation coefficient)
  - ê³µë¶„ì‚°ì„ ë‘ ë³€ìˆ˜ì˜ í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ ì¤€ ê°’
  > $r_{x, y} = \frac{cov(X,Y)}{\sigma_{X}\sigma_{Y} }$
  - ê³µë¶„ì‚°ì˜ ìŠ¤ì¼€ì¼ì„ ì¡°ì •í•˜ëŠ” íš¨ê³¼
  - ë³€ìˆ˜ì˜ ìŠ¤ì¼€ì¼ì— ì˜í–¥ì„ ë°›ì§€ ì•ŠìŒ
  - -1ì—ì„œ 1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§
  - ìƒê´€ê³„ìˆ˜ê°€ 1ì´ë¼ëŠ” ê²ƒì€ í•œ ë³€ìˆ˜ê°€ ë‹¤ë¥¸ ë³€ìˆ˜ì— ëŒ€í•´ì„œ ì™„ë²½í•œ ì–‘ì˜ ì„ í˜• ê´€ê³„ë¥¼ ê°–ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸

  - [df.corr()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html) ë˜ëŠ” [np.corrcoef()](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html)  

  - numpyë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° dfë¥¼ transpose í•œ ì´ìœ  [[ê³µì‹ë¬¸ì„œ numpy.corrcoef]](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html)
    - numpy.corrcoef(x, y=None, rowvar=True, bias=<no value>, ddof=<no value>, *, dtype=None) 
      - Parameters
        - rowvarbool, optional
          - If rowvar is True (default), then each row represents a variable, with observations in the columns. Otherwise, the relationship is transposed: each column represents a variable, while the rows contain observations.
            - rowvar íŒŒë¼ë¯¸í„°ëŠ” ê¸°ë³¸ê°’ì´ `True`ë¡œ ë˜ì–´ìˆìŒ
            - ê³§, ê° í–‰ì´ ë³€ìˆ˜, ì—´ì´ ê´€ì¸¡ê°’ì´ë¼ëŠ” ëœ»
            - ë°˜ëŒ€(`False`)ì´ë©´ ì „ì¹˜(transpose) ë¨

#### ì„ í˜•ëŒ€ìˆ˜(Linear Algebra)
- Vector Transformation
    - ë²¡í„°ì˜ ë³€í™˜ì€ ë²¡í„°ì™€ í–‰ë ¬  T ì˜ ê³±
    - np.matmul(ë©”íŠ¸ë¦­ìŠ¤, ë²¡í„°)  
- Eigenstuff
    - EigenstuffëŠ” np.linalg.eig()ì„ ì‚¬ìš©í•˜ì—¬ êµ¬í•  ìˆ˜ ìˆìŒ
    - np.linalg.eig() ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ eigenstuffëŠ” 2ê°œì˜ ë°°ì—´ì„ ê°–ìŒ
    - ì²«ë²ˆì§¸ ë°°ì—´ì€ Eigenvalue, ë‘ë²ˆì§¸ ë°°ì—´ì€ Eigenvector
    - Eigenvalue ì˜ ì¸ë±ìŠ¤, Eigenvector ì˜ columnì´ ìŒì„ ì´ë£¨ëŠ” eigenstuff ê°€ ë¨
    - eigenstuff ëŠ” ì£¼ì–´ì§„ ë©”íŠ¸ë¦­ìŠ¤ì˜ column ê°¯ìˆ˜ ë§Œí¼ ìƒì„± ë¨
- ê³ ìœ ë²¡í„° Eigenvector
    - ì£¼ì–´ì§„ transformationì— ì˜í•´ì„œ í¬ê¸°ë§Œ ë³€í•˜ê³  ë°©í–¥ì€ ë³€í•˜ì§€ ì•ŠëŠ” ë²¡í„°
    - ì •ë°©í–‰ë ¬ Aì— ëŒ€í•˜ì—¬ Ax = Î»x  (ìƒìˆ˜ Î») ê°€ ì„±ë¦½í•˜ëŠ” 0ì´ ì•„ë‹Œ ë²¡í„° xê°€ ì¡´ì¬í•  ë•Œ ìƒìˆ˜ Î» ë¥¼ í–‰ë ¬ Aì˜ ê³ ìœ ê°’ (eigenvalue), x ë¥¼ ì´ì— ëŒ€ì‘í•˜ëŠ” ê³ ìœ ë²¡í„° (eigenvector) ë¼ê³  í•¨
    - Eigenvector ëŠ” í¬ê¸°ê°€ 1ì¸ ë‹¨ìœ„ë²¡í„°
- ê³ ìœ ê°’ Eigenvalue
    - Eigenvectorì˜ ë³€í™”í•œ í¬ê¸° ê°’


#### ì£¼ì„±ë¶„ ë¶„ì„(Principal Component Analysis, PCA)  
- ì£¼ì„±ë¶„ë¶„ì„
  - ì›ë˜ ë°ì´í„°ì˜ ì •ë³´(ë¶„ì‚°)ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ëŠ” ìƒˆë¡œìš´ ì¶•ì„ ì°¾ê³ , ê·¸ ì¶•ì— ë°ì´í„°ë¥¼ ì‚¬ì˜(Linear Projection)í•˜ì—¬ ê³ ì°¨ì›ì˜ ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ë²•
  - ì£¼ì„±ë¶„(PC)ì€ ê¸°ì¡´ ë°ì´í„°ì˜ ë¶„ì‚°ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ë„ë¡ ë°ì´í„°ë¥¼ projection í•˜ëŠ” ì¶•
  - PCì˜ ë‹¨ìœ„ë²¡í„°ëŠ” ë°ì´í„°ì˜ ê³µë¶„ì‚° í–‰ë ¬ì— ëŒ€í•œ eigenvector
  - eigenvectorì— projectioní•œ ë°ì´í„°ì˜ ë¶„ì‚°ì´ eigenvalue
  - Feature Extraction ë°©ì‹ ì¤‘ í•˜ë‚˜
  - ê¸°ì¡´ ë³€ìˆ˜ì˜ ì„ í˜•ê²°í•©ìœ¼ë¡œ ìƒˆë¡œìš´ ë³€ìˆ˜(PC)ë¥¼ ìƒì„±
- PCAê°€ í•„ìš”í•œ ì´ìœ 
  - ê³ ì°¨ì›ì˜ ë¬¸ì œ(The Curse of Dimensionality; ì°¨ì›ì˜ ì €ì£¼)
    - sns.pairplot()
  - ë¹„íš¨ìœ¨ì„±, Overfitting(ê³¼ì í•©)
  - ì°¨ì› ì¶•ì†Œ(Dimensionality Reduction)
  - Feature selection
    - ë°ì´í„°ì…‹ì—ì„œ ëœ ì¤‘ìš”í•œ featureëŠ” ì œê±°í•˜ëŠ” ë°©ë²•
    - ë¶„ì„ ëª©ì ì— ì í•©í•œ ì†Œìˆ˜ì˜ ê¸°ì¡´ featureë¥¼ ì„ íƒ
    - featureì˜ í•´ì„ì´ ì‰½ë‹¤ëŠ” ì¥ì  
    - feature ê°„ ì—°ê´€ì„±ì„ ì§ì ‘ ê³ ë ¤í•´ì•¼
  - Feature extraction
    - ê¸°ì¡´ featureë¥¼ ì¡°í•©í•˜ì—¬ ì‚¬ìš©
    - feature ê°„ ìƒê´€ê´€ê³„ë¥¼ ê³ ë ¤í•˜ì—¬ ì¡°í•©
    - ìƒˆë¡œìš´ featureê°€ ì¶”ì¶œ(ìƒì„±)  
    - ì›ë˜ ë³€ìˆ˜ë“¤ì˜ ì„ í˜•ê²°í•©ìœ¼ë¡œ ì´ë£¨ì–´ì§
    ex) ì§‘ê°’ ë°ì´í„°ì…‹ì—ì„œ `ì§‘ ë©´ì `, `ë°© ê°œìˆ˜`, `í™”ì¥ì‹¤ ê°œìˆ˜`ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜ë¥¼ ì¡°í•©í•˜ì—¬ `í¬ê¸°`ë¼ëŠ” í•˜ë‚˜ì˜ ë³€ìˆ˜ë¥¼ ì‚¬ìš© 
    - featureì˜ í•´ì„ì´ ì–´ë ¤ì›€ 
    - feature ìˆ˜ë¥¼ ë§ì´ ì¤„ì¼ ìˆ˜ ìˆìŒ   



#### PCA ìˆœì„œ
- EDA
- ë°ì´í„° í‘œì¤€í™”
- ê³µë¶„ì‚° í–‰ë ¬ êµ¬í•˜ê¸°
- Eigensruff êµ¬í•˜ê¸°
- Projection Data to Eigenvector

#### sklearn ì„ í™œìš©í•œ PCA

#### Scree Plots
- scree plotsì„ í™œìš©í•´ ì£¼ì„±ë¶„ì˜ ê°¯ìˆ˜ ê²°ì •
    - ì•„ë˜ ê·¸ë¦¼ì—ì„œ ì£¼ì„±ë¶„ì´ ë‘ê°œê°€ ë˜ì—ˆì„ ë•Œ ëˆ„ì ë¶„ì‚°ì´ 88.2%ê°€ ë˜ì–´ ì ì ˆí•œ ê°’ìœ¼ë¡œ ë³´ì„
    - ì¼ë°˜ì ìœ¼ë¡œ ëˆ„ì ë¶„ì‚°ì´ 70~80% ì´ìƒì´ë©´ ì ì ˆí•˜ë‹¤ê³  ì—¬ê²¨ì§

![image](../../assets/images/aib/section1/scree_plot.png){: .align-center width="70%"}  


- ë‘ ê°œì˜ ì£¼ì„±ë¶„ìœ¼ë¡œ ë§Œë“  ì‚°ì ë„
    - Xì¶•ì€ ì²«ë²ˆì§¸ ì£¼ì„±ë¶„, Yì¶•ì€ ë‘ë²ˆì§¸ ì£¼ì„±ë¶„

![image](../../assets/images/aib/section1/kmeans_clustering.png){: .align-center width="70%"}  


- ì„¸ ê°œì˜ ì£¼ì„±ë¶„ìœ¼ë¡œ ë§Œë“  ì‚°ì ë„
    - ì²« ë²ˆì§¸ ì£¼ì„±ë¶„(PC1), ë‘ ë²ˆì§¸ ì£¼ì„±ë¶„(PC2), ì„¸ ë²ˆì§¸ ì£¼ì„±ë¶„(PC3)
    - ì„¸ ë²ˆì§¸ ì£¼ì„±ë¶„ì€, 2Dë¡œ í‘œí˜„í•œ ì¢Œí‘œí‰ë©´ì˜ ì‚°ì ë„ë³´ë‹¤, chinstrap í­ê·„ì˜ ìœ„ì¹˜ë¥¼ ë”ìš± ì…ì²´ì ìœ¼ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.

![image](../../assets/images/aib/section1/3k_penguin.png){: .align-center width="70%"}  


<br>

### Clustering

- ë°ì´í„° ì‚¬ì´ì˜ ê±°ë¦¬ ê³„ì‚°í•˜ëŠ” ë°©ë²•ë“¤
   - [Euclidean Distance](https://www.cuemath.com/euclidean-distance-formula/)
   - [Cosine Similarity](https://www.learndatasci.com/glossary/cosine-similarity/)
   - [Jaccard Distance](https://www.statology.org/jaccard-similarity/)

- Hierarchical Clustering Dendrogram  
![image](../../assets/images/aib/section1/h_cluster.png){: .align-center width="70%"}  
   
- ì‹¤ì œìë£Œ, K-Means Clustering, Hierarchical Clustering ë¹„êµ  
![image](../../assets/images/aib/section1/clusterings.png){: .align-center width="90%"}  

- diagnosis - K Means cluser ê²°í•© ë¹„ìœ¨      = 93.5%
- diagnosis - Hierarchical cluser ê²°í•© ë¹„ìœ¨ = 91.9%
- K-Means Clusteringì´ ê·¼ì†Œí•˜ê²Œ ê²°í•©ë¥ ì´ ë†’ìŒ

<br>

### ë¯¸ë¶„, ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)

- ì¼ë°˜ì ì¸ ë¯¸ë¶„ ê³µì‹  
$$f'(x) = {f(x + \Delta x) - f(x) \over \Delta x}, \Delta x \rightarrow 0$$

- $\Delta x$ëŠ” 0ì´ ì˜¬ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— 0ì— ë§¤ìš° ê·¼ì ‘í•œ ê°’ì„ ì‚¬ìš©í•˜ê²Œ ë¨
- ë³´í†µ $\Delta x$ëŠ” $1e-5$ê°€ ìì£¼ ì‚¬ìš©ë¨
- ì´ë ‡ê²Œ $\Delta x$ë¡œ 0ì— ê·¼ì ‘í•œ ìˆ˜ $(1e-5)$ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì´ë¥¼ `numerical method`ë¼ê³  í‘œí˜„í•¨
- `numerical method`ì—ì„œëŠ” ì¡°ê¸ˆ ë” ì •í™•í•œ ì¸¡ì •ì„ ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°
$$f'(x) = {f(x + \Delta x) - f(x - \Delta x) \over 2\Delta x}$$

##### ë¯¸ë¶„ê³µì‹
###### ê¸°ë³¸ ë¯¸ë¶„ê³µì‹
-  $y = c$ (cëŠ” ìƒìˆ˜) $ \rightarrow $ $  y' = 0$  
-  $y = x^n$ (nì€ ìì—°ìˆ˜) $ \Rightarrow $ $  y' = nx^{n-1}$  
-  $y = cf(x)$ (cëŠ” ìƒìˆ˜) $ \Rightarrow $ $  y' = cf'(x)$  
-  $y = f(x) \pm g(x)$ $ \Rightarrow$ $  y' = f(x) \pm g(x)$ (ë³µë¶€í˜¸ ë™ìˆœ)  
-  $y = f(x)g(x)$ $ \Rightarrow$ $  y' = f'(x)g(x) + f(x)g'(x)$ (ê³±ì…ˆë²•ì¹™)  
-  $y = f(x)g(x)h(x)$ $ \Rightarrow$ $  y' = f'(x)g(x)h(x) + f(x)g'(x)h(x) + f(x)g(x)h'(x)$  
-  $y = f(g(x))$ $ \Rightarrow$ $  y' = f'(x)g'(x)$ (ì—°ì‡„ë²•ì¹™)  

- ì§€ìˆ˜í•¨ìˆ˜  
  - $y = e^x $ $\Rightarrow$ $  y' = e^x$  
  - $y = a^x $ $\Rightarrow$ $  y' = a^x \ln a$ $(ë‹¨, a>0,\ a \neq 1)$  
- ë¡œê·¸í•¨ìˆ˜  
  - $y = \log{x}$ $\Rightarrow$ $y' = { {1} \over {x} } $  

###### ì„ í˜•ì¡°í•©ë²•ì¹™

- $y = cf(x) + cg(x)  â‡’  y' = cf'(x) + cg'(x)$

###### ê³±ì…ˆë²•ì¹™

- $y = f(x)g(x)  â‡’  y' = f(x)g'(x) + f'(x)g(x)$

- ì˜ˆì‹œ : $y = xe^x  â‡’  y' = xe^x + e^x$

###### ì—°ì‡„ë²•ì¹™ chain rule
- $y = f(g(x))$ $ \Rightarrow$ $  y' = f'(g)g'(x)$
- ì˜ˆ : ì •ê·œë¶„í¬ì˜ í™•ë¥ ë°€ë„í•¨ìˆ˜
  - $y = \exp { {(x - \mu)}^2 \over {\sigma}^2 }$
  - ìœ„ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ì„
    - $y = f(g(h(x)))$
    - $f(g) = \exp(g)$
    - $g(h) = {h^2 \over {\sigma}^2}$
    - $h(x) = x - \mu$
  - ì—°ì‡„ë²•ì¹™ì„ ì ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŒ
    - $y' = f'(g)\cdot g'(h)\cdot h'(x)$
    - $f'(g) = \exp(g) = \exp { {(x - \mu)}^2 \over {\sigma}^2 }$
    - $g'(h) = {2h \over {\sigma}^2} = {2(x - \mu) \over {\sigma}^2}$
    - $h'(x) = 1$
    - $y' = \exp { {(x - \mu)}^2 \over {\sigma}^2}\cdot {2(x - \mu) \over {\sigma}^2}\cdot 1$

- ì˜ˆ : ë¡œê·¸í•¨ìˆ˜
  - $y = \log(x^2 - 3k)$
    - $y' = {1 \over (x^2 - 3k)} \cdot 2x = {2x \over (x^2 - 3k)}$

##### 2ì°¨ ë„í•¨ìˆ˜
- ë„í•¨ìˆ˜ë¥¼ í•œ ë²ˆ ë” ë¯¸ë¶„í•˜ì—¬ ë§Œë“¤ì–´ì§„ í•¨ìˆ˜ë¥¼ **2ì°¨ ë„í•¨ìˆ˜(second derivative)**ë¼ê³  í•œë‹¤.

##### í¸ë¯¸ë¶„
- ë§Œì•½ í•¨ìˆ˜ê°€ ë‘˜ ì´ìƒì˜ ë…ë¦½ë³€ìˆ˜ë¥¼ ê°€ì§€ëŠ” ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì¸ ê²½ìš°ì—ë„ ë¯¸ë¶„ ì¦‰, ê¸°ìš¸ê¸°ëŠ” í•˜ë‚˜ì˜ ë³€ìˆ˜ì— ëŒ€í•´ì„œë§Œ êµ¬í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ **í¸ë¯¸ë¶„(partial differentiation)**ì´ë¼ê³  í•œë‹¤.
- í¸ë¯¸ë¶„ì„ í•˜ëŠ” ë°©ë²•ì€ ë³€ìˆ˜ê°€ í•˜ë‚˜ì¸ í•¨ìˆ˜ì˜ ë¯¸ë¶„ê³¼ ê°™ë‹¤. ë‹¤ë§Œ ì–´ë–¤ í•˜ë‚˜ì˜ ë…ë¦½ ë³€ìˆ˜ì— ëŒ€í•´ ë¯¸ë¶„í•  ë•ŒëŠ” ë‹¤ë¥¸ ë…ë¦½ ë³€ìˆ˜ë¥¼ ìƒìˆ˜ë¡œ ìƒê°í•˜ë©´ ëœë‹¤.

##### í…Œì¼ëŸ¬ ì „ê°œ
- í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°(1ì°¨ ë¯¸ë¶„ê°’)ë¥¼ ì•Œê³  ìˆë‹¤ë©´ í•¨ìˆ˜ì˜ ëª¨ì–‘ì„ ë‹¤ìŒì²˜ëŸ¼ ê·¼ì‚¬í™”í•  ìˆ˜ ìˆë‹¤. x0ëŠ” í•¨ìˆ˜ê°’ê³¼ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ëŠ” x ìœ„ì¹˜ì´ë©° ì‚¬ìš©ìê°€ ë§ˆìŒëŒ€ë¡œ ì„¤ì •í•  ìˆ˜ ìˆë‹¤.  
  - $f(x) \approx f(x_0) + {df(x_0) \over dx}(x - x_0)$
- ì´ë¥¼ **í…Œì¼ëŸ¬ ì „ê°œ(Taylor expansion)**ë¼ê³  í•œë‹¤. ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê²½ìš°ì—ëŠ” ë‹¤ìŒì²˜ëŸ¼ í…Œì¼ëŸ¬ ì „ê°œë¥¼ í•œë‹¤.
  - $f(x,y) \approx f(x_0,y_0) + {\partial df(x_0,y_0) \over \partial dx}(x - x_0) + {\partial df(x_0,y_0) \over \partial dx}(y - y_0)$

##### ë¯¸ë¶„ê²°ê³¼ ì‹œê°í™”
- $f(x) = e^x$ ì˜ ê·¸ë˜í”„  

![image](../../assets/images/aib/section1/e_prime.png){: .align-center width="70%"}  


- $f(x) = lnx$ ì˜ ê·¸ë˜í”„  

![image](../../assets/images/aib/section1/ln_prime.png){: .align-center width="70%"} 



- $f(x) = 2x^2 + 8x$ ì˜ ê·¸ë˜í”„  

![image](../../assets/images/aib/section1/f_prime.png){: .align-center width="70%"}  


- $\sigma(x) = { {1} \over {1 + e^{-x} } }$ ì˜ ê·¸ë˜í”„  

![image](../../assets/images/aib/section1/sig_prime.png){: .align-center width="70%"} 

 
### Gradient Descent
- ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì˜ ëŒ€í‘œ  
- ë…ë¦½ë³€ìˆ˜ $\theta_1$ê³¼ $\theta_2$ì„ ë³€í˜•ì‹œì¼œê°€ë©° ì˜¤ì°¨ í•¨ìˆ˜ $J(\theta_1, \theta_2)$ ì˜ ìµœì†Œê°’ì„ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜  
  - ì˜¤ì°¨ í•¨ìˆ˜ $J(\theta_1, \theta_2)$ ì˜ ìµœì†Œê°’ì€ ê¸°ìš¸ê¸°ê°€ 0ì¸ ì§€ì   
  - ê¸°ìš¸ê¸°ê°€ ì–‘ìˆ˜ë¼ëŠ” ê²ƒì€ $\theta$ ê°’ì´ ì»¤ì§ˆ ìˆ˜ë¡ í•¨ìˆ˜ ê°’ì´ ì»¤ì§„ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ê³ , ë°˜ëŒ€ë¡œ ê¸°ìš¸ê¸°ê°€ ìŒìˆ˜ë¼ë©´ $\theta$ê°’ì´ ì»¤ì§ˆìˆ˜ë¡ í•¨ìˆ˜ì˜ ê°’ì´ ì‘ì•„ì§„ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸  

- ì˜¤ì°¨í•¨ìˆ˜
  - $Error Function = (yÌ‚^{(i)} - y^{(i)})^2$
    - $yÌ‚^{(i)}$ : i ë²ˆì§¸ ì‚¬ë¡€ì˜ ì˜ˆì¸¡ ê°’
    - $y^{(i)}$ : i ë²ˆì§¸ ì‚¬ë¡€ì˜ ë¼ë²¨
- ë¹„ìš©í•¨ìˆ˜
  - $MSE(Î¸,b) = {1 \over m}Î£(yÌ‚^{(i)} - y^{(i)})^2$  
  $   = {1 \over m}Î£(Î¸x^{(i)}  + b - y^{(i)})^2$
    - $yÌ‚^{(i)}$ : i ë²ˆì§¸ ì‚¬ë¡€ì˜ ì˜ˆì¸¡ ê°’
    - $y^{(i)}$ : i ë²ˆì§¸ ì‚¬ë¡€ì˜ ë¼ë²¨
    - $x^{(i)}$ : i ë²ˆì§¸ ì…ë ¥ ë°ì´í„° ë²¡í„°
    - $Î¸$ : ê°€ì¤‘ì¹˜ ë²¡í„° (ê¸°ìš¸ê¸°)
    - $b$ : í¸ì°¨ (yì ˆí¸)
- ë¹„ìš©í•¨ìˆ˜ë¥¼ ë¯¸ë¶„
  - $Î¸ í¸ë¯¸ë¶„$
    - ${\partial \over \partial \theta}MSE(Î¸,b) = {1 \over m}Î£[(Î¸x^{(i)}  + b - y^{(i)})^2]'$  
    $   = {2 \over m}Î£(Î¸x^{(i)}  + b - y^{(i)})[(Î¸x^{(i)}  + b - y^{(i)})]'$  
    $   = {2 \over m}Î£(Î¸x^{(i)}  + b - y^{(i)})x^{(i)}$  
  - $b í¸ë¯¸ë¶„$
    - ${\partial \over \partial b}MSE(Î¸,b) = {1 \over m}Î£[(Î¸x^{(i)}  + b - y^{(i)})^2]'$  
    $   = {2 \over m}Î£(Î¸x^{(i)}  + b - y^{(i)})[(Î¸x^{(i)}  + b - y^{(i)})]'$  
    $   = {2 \over m}Î£(Î¸x^{(i)}  + b - y^{(i)})$  

- ê²½ì‚¬í•˜ê°• ì•Œê³ ë¦¬ì¦˜ ê³¼ì •  
  1. ê²½ì‚¬í•˜ê°•ë²•ì€ ì„ì˜ì˜ $\theta_1, \theta_2$ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒ ì¦‰, random initializationì„ ì‹¤í–‰
  2. ë°˜ë³µì ìœ¼ë¡œ íŒŒë¼ë¯¸í„° $\theta_1, \theta_2$ë¥¼ ì—…ë°ì´íŠ¸ í•´ê°€ë©°, ì˜¤ì°¨ í•¨ìˆ˜ $J(\theta_1,\theta_2)$ ê°’ì´ ë‚®ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰  
  3. ê¸°ìš¸ê¸°ê°€ ì»¤ì§„ë‹¤ëŠ” ê²ƒì€ ì˜¤ì°¨í•¨ìˆ˜ ê°’ì´ ì»¤ì§€ëŠ” ë°©í–¥ì´ë¼ëŠ” ê²ƒê³¼ ê°™ê¸° ë•Œë¬¸ì— ê²½ì‚¬í•˜ê°•ë²• ì•Œê³ ë¦¬ì¦˜ì€ ê¸°ìš¸ê¸°ì˜ ë°˜ëŒ€ ë°©í–¥(ê¸°ìš¸ê¸° $âˆ‡ J(\theta_1)$, $âˆ‡ J(\theta_2)$ê°€ ì‘ì•„ì§€ëŠ” ë°©í–¥)ìœ¼ë¡œ ì´ë™  
  4. ê·¸ë¦¬ê³  ê¸°ìš¸ê¸°ê°€ 0ì´ ë˜ì–´ **`global minimum`**ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ì´ë™  
- ê²½ì‚¬í•˜ê°• ì•Œê³ ë¦¬ì¦˜ ìˆ˜í•™ì  ê³µì‹
$$\theta_{n+1} = \theta_n - \eta âˆ‡ J(\theta_n)$$
  - $\eta$ëŠ” í•™ìŠµë¥ , $âˆ‡ J(\theta_n)$ëŠ” ê¸°ìš¸ê¸°ë¥¼ ì˜ë¯¸

- í¸ë¯¸ë¶„ - ì „ë¯¸ë¶„ì˜ ì°¨ì´
  - í¸ë¯¸ë¶„ : í•˜ë‚˜ì˜ ë³€ìˆ˜ë¥¼ ê³ ë ¤í•œ í•˜ë‚˜ì˜ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°  
  - ì „ë¯¸ë¶„ : ë‹¤ì–‘í•œ ë³€ìˆ˜ì— ëŒ€í•´ ë‹¤ì–‘í•œ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°  
    - ê°ê°ì˜ ë³€ìˆ˜ì— ëŒ€í•œ í¸ë¯¸ë¶„ ë’¤ ëª¨ë‘ ë”í•˜ê¸° = ì¦ë¶„


![image](../../assets/images/aib/section1/gradient_descent.gif){: .align-center width="70%"} 



<br><br><br><br>  
<center>  
<h1>ëê¹Œì§€ ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤ğŸ˜‰</h1>  
</center>  
<br><br><br><br>  